{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NQR-MPbJD_Jp"
      },
      "outputs": [],
      "source": [
        "### Intro to Deep Learning\n",
        "\n",
        "## 1. A Single Neuron\n",
        "\n",
        "# Deep learning is an approach to machine learning characterized by deep stacks of computations.\n",
        "# The power of a neural network comes from the complexity of the connections neurons form.\n",
        "# Neural networks are composed of neurons, where each neuron individually performs only a simple computation.\n",
        "\n",
        "# The Linear Unit: y = wx + b\n",
        "# w: weight, b: bias\n",
        "# Multiple Inputs: y = w0x0 + w1x1 + w2x2 + b"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Linear Units in Keras\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "#Create a network with 1 linear uit\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(units=1, input_shape=[3])\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtHKjIAYJada",
        "outputId": "2c2eec1b-9dad-430e-ba24-05dd8fa4ef60"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 2. Deep Neural Networks\n",
        "# When we collect together linear units having a common set of inputs we get a dense layer.\n",
        "# A layer can be any kind of data transformation.\n",
        "# Many layers, like the convolutional and recurrent layers, transform data through use of neurons and differ primarily in the pattern of connections they form.\n",
        "\n",
        "# The Activation Fuction\n",
        "# However two dense layers with nothing in between are no better than a single dense layer by itself.\n",
        "# What we need are activation functions that move us out of the world of lines and planes.\n",
        "# An activation function is simply some function we apply to each of a layer's outputs.\n",
        "# ex) rectified linear unit = ReLU\n",
        "\n",
        "# Stacking Dense Layers\n",
        "# The layers before the final(output) layer are hidden layers. We never see their outputs directly.\n",
        "# In regression tasks, the final layer is a linear unit(=no activation function)\n",
        "# Other tasks like classification might require an activation function on the output."
      ],
      "metadata": {
        "id": "YNyM-jjuJ4wG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Building Sequential Models\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "model = keras.Sequential([\n",
        "    # the hidden ReLU layers\n",
        "    layers.Dense(units=4, activation='relu', input_shape=[2]),\n",
        "    layers.Dense(units=3, activation='relu'),\n",
        "    # the linear output layer\n",
        "    layers.Dense(units=1)\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9q4BYxLnN4rc",
        "outputId": "0a9438f5-621d-4b90-b6de-56d97751ec9f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 3. Stochastic Gradient Descent\n",
        "# The Loss Function\n",
        "# The loss function measures the disparity between the target's true value and the value the model predicts.\n",
        "# A common loss function for regression problem is the mean absolute error, MAE.\n",
        "# MAE measures the disparsity from the true target y_true by an absolute difference abs(y_true - y_pred).\n",
        "# During training, the model will use the loss function as a guide for finding the correct values of its weights.\n",
        "\n",
        "# The Optimizer - Stochastic Gradient Descent\n",
        "# The optimizer is an algorithm that adjusts the weights to minimize the loss.\n",
        "# Adam is a great general-purpose optimizer.\n",
        "# The learning rate and the size of the minibatches are the two parameters that have the largest effect on how the SGD training proceeds."
      ],
      "metadata": {
        "id": "mvEXag9jOVUO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding the Loss and Optimizer\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='mae'\n",
        ")"
      ],
      "metadata": {
        "id": "qRPAQBZeSjw9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example - Red Wine Quality\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "red_wine = pd.read_csv('../input/dl-course-data/red-wine.csv')\n",
        "\n",
        "# Create training and validation splits\n",
        "df_train = red_wine.sample(frac=0.7, random_state=0)\n",
        "df_valid = red_wine.drop(df_train.index)\n",
        "display(df_train.head(4))\n",
        "\n",
        "# Scale to [0, 1]\n",
        "max_ = df_train.max(axis=0)\n",
        "min_ = df_train.min(axis=0)\n",
        "df_train = (df_train - min_) / (max_ - min_)\n",
        "df_valid = (df_valid - min_) / (max_ - min_)\n",
        "\n",
        "# Split features and target\n",
        "X_train = df_train.drop('quality', axis=1)\n",
        "X_valid = df_valid.drop('quality', axis=1)\n",
        "y_train = df_train['quality']\n",
        "y_valid = df_valid['quality']\n",
        "\n",
        "print(X_train.shape)\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(512, activation='relu', input_shape=[11]),\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.Dense(1),\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='mae',\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_valid, y_valid),\n",
        "    batch_size=256,\n",
        "    epochs=10,\n",
        ")\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# convert the training history to a dataframe\n",
        "history_df = pd.DataFrame(history.history)\n",
        "# use Pandas native plot method\n",
        "history_df['loss'].plot();"
      ],
      "metadata": {
        "id": "vNi8zorWTAoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 4. Overfitting and Underfitting\n",
        "\n",
        "# Interpreting the Learning Curves\n",
        "# Underfitting the training set is when the loss is not as low as it could be b/c the model hasn't learned enough signal.\n",
        "# Overfitting the training set is when the loss is not as low as it could be b/c the model learned too much noise.\n",
        "\n",
        "# A model's capacity refers to the size and complexity of the patterns it is able to learn.\n",
        "# underfitted -> try increasing its capacity\n",
        "# Increase capacity of a network by making it wider(more units to existing layers) or deeper(adding more layers)\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(16, activation='relu'),\n",
        "    layers.Dense(1),\n",
        "])\n",
        "\n",
        "wider = keras.Sequential([\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(1),\n",
        "])\n",
        "\n",
        "deeper = keras.Sequential([\n",
        "    layers.Dense(16, activation='relu'),\n",
        "    layers.Dense(16, activation='relu'),\n",
        "    layers.Dense(1),\n",
        "])\n",
        "\n",
        "# Early Stopping\n",
        "# When a model is too eagerly learning noise, the validation loss may start to increase during training.\n",
        "# To prevent this, early stopping can stop training whenever it seems the validation loss isn't decreasing anymore.\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stopping = EarlyStopping(\n",
        "    min_delta=0.001, # minimium amount of change to count as an improvement\n",
        "    patience=20, # how many epochs to wait before stopping\n",
        "    restore_best_weights=True,\n",
        ")"
      ],
      "metadata": {
        "id": "Pw2D5TSfTada"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7OgHDx7dWMcW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example - Train a Model with Early Stopping\n",
        "\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "red_wine = pd.read_csv('../input/dl-course-data/red-wine.csv')\n",
        "\n",
        "# Create training and validation splits\n",
        "df_train = red_wine.sample(frac=0.7, random_state=0)\n",
        "df_valid = red_wine.drop(df_train.index)\n",
        "display(df_train.head(4))\n",
        "\n",
        "# Scale to [0, 1]\n",
        "max_ = df_train.max(axis=0)\n",
        "min_ = df_train.min(axis=0)\n",
        "df_train = (df_train - min_) / (max_ - min_)\n",
        "df_valid = (df_valid - min_) / (max_ - min_)\n",
        "\n",
        "# Split features and target\n",
        "X_train = df_train.drop('quality', axis=1)\n",
        "X_valid = df_valid.drop('quality', axis=1)\n",
        "y_train = df_train['quality']\n",
        "y_valid = df_valid['quality']\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, callbacks\n",
        "\n",
        "early_stopping = callbacks.EarlyStopping(\n",
        "    min_delta=0.001, # minimium amount of change to count as an improvement\n",
        "    patience=20, # how many epochs to wait before stopping\n",
        "    restore_best_weights=True,\n",
        ")\n",
        "\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(512, activation='relu', input_shape=[11]),\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.Dense(1),\n",
        "])\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='mae',\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_valid, y_valid),\n",
        "    batch_size=256,\n",
        "    epochs=500,\n",
        "    callbacks=[early_stopping], # put your callbacks in a list\n",
        "    verbose=0,  # turn off training log\n",
        ")\n",
        "\n",
        "history_df = pd.DataFrame(history.history)\n",
        "history_df.loc[:, ['loss', 'val_loss']].plot();\n",
        "print(\"Minimum validation loss: {}\".format(history_df['val_loss'].min()))"
      ],
      "metadata": {
        "id": "M81SClYfV7xv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 5. Dropout and Batch Normalization\n",
        "# Dropout layer can help correct overfitting.\n",
        "keras.Sequential([\n",
        "    # ...\n",
        "    layers.Dropout(rate=0.3), # apply 30% dropout to the next layer\n",
        "    layers.Dense(16),\n",
        "    # ...\n",
        "])\n",
        "\n",
        "# Batch Normalization\n",
        "# A batch normalization layer looks at each batch as it comes in, first normalizaing the batch with its own mean and standard deviation,\n",
        "# and then also putting the data on a new scale with two trainable rescaling parameters.\n",
        "# Most often, batchnorm is added as an aid to the optimization process.\n",
        "# after a layer:\n",
        "layers.Dense(16, activation='relu'),\n",
        "layers.BatchNormalization(),\n",
        "# between a layer and its activation function:\n",
        "layers.Dense(16),\n",
        "layers.BatchNormalization(),\n",
        "layers.Activation('relu'),"
      ],
      "metadata": {
        "id": "hzJ2xiI0WWv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example - Using Dropout and Batch Normalization\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(1024, activation='relu', input_shape=[11]),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dense(1024, activation='relu'),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dense(1024, activation='relu'),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dense(1),\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='mae',\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_valid, y_valid),\n",
        "    batch_size=256,\n",
        "    epochs=100,\n",
        "    verbose=0,\n",
        ")\n",
        "\n",
        "\n",
        "# Show the learning curves\n",
        "history_df = pd.DataFrame(history.history)\n",
        "history_df.loc[:, ['loss', 'val_loss']].plot();"
      ],
      "metadata": {
        "id": "SauEIT5tYfDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 6. Binary Classification\n",
        "# Accuracy is the ratio of correct predictions to total predictions:\n",
        "# accuracy = number_correct / total\n",
        "# Cross-entropy is a sort of measure for the distance from one probability distribution to another.\n",
        "# The further away the predicted probability is from 1.0, the greater will be the cross-entropy loss.\n",
        "\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "ion = pd.read_csv('../input/dl-course-data/ion.csv', index_col=0)\n",
        "display(ion.head())\n",
        "\n",
        "df = ion.copy()\n",
        "df['Class'] = df['Class'].map({'good': 0, 'bad': 1})\n",
        "\n",
        "df_train = df.sample(frac=0.7, random_state=0)\n",
        "df_valid = df.drop(df_train.index)\n",
        "\n",
        "max_ = df_train.max(axis=0)\n",
        "min_ = df_train.min(axis=0)\n",
        "\n",
        "df_train = (df_train - min_) / (max_ - min_)\n",
        "df_valid = (df_valid - min_) / (max_ - min_)\n",
        "df_train.dropna(axis=1, inplace=True) # drop the empty feature in column 2\n",
        "df_valid.dropna(axis=1, inplace=True)\n",
        "\n",
        "X_train = df_train.drop('Class', axis=1)\n",
        "X_valid = df_valid.drop('Class', axis=1)\n",
        "y_train = df_train['Class']\n",
        "y_valid = df_valid['Class']\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(4, activation='relu', input_shape=[33]),\n",
        "    layers.Dense(4, activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid'),\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['binary_accuracy'],\n",
        ")\n",
        "\n",
        "early_stopping = keras.callbacks.EarlyStopping(\n",
        "    patience=10,\n",
        "    min_delta=0.001,\n",
        "    restore_best_weights=True,\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_valid, y_valid),\n",
        "    batch_size=512,\n",
        "    epochs=1000,\n",
        "    callbacks=[early_stopping],\n",
        "    verbose=0, # hide the output because we have so many epochs\n",
        ")\n",
        "\n",
        "history_df = pd.DataFrame(history.history)\n",
        "# Start the plot at epoch 5\n",
        "history_df.loc[5:, ['loss', 'val_loss']].plot()\n",
        "history_df.loc[5:, ['binary_accuracy', 'val_binary_accuracy']].plot()\n",
        "\n",
        "print((\"Best Validation Loss: {:0.4f}\" +\\\n",
        "      \"\\nBest Validation Accuracy: {:0.4f}\")\\\n",
        "      .format(history_df['val_loss'].min(),\n",
        "              history_df['val_binary_accuracy'].max()))"
      ],
      "metadata": {
        "id": "DbooOE4NYsaI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}