{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7장 시계열 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1 시계열 문제\n",
    "\"\"\"\n",
    "불규칙 변동/추세 변동/순환 변동/계절 변동\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.2 AR, MA, ARMA, ARIMA\n",
    "\"\"\"\n",
    "시계열 분석은 독립 변수를 사용하여 종속 변수를 예측하는 일반적인 머신 러닝에서 시간을 독립 변수로 사용.\n",
    "독립 변수로 시간을 사용 -> AR/MA/ARMA/ARIMA 모델\n",
    "\n",
    "7.2.1. AR(AutoRegressive)\n",
    "이전 관측 값이 이후 관측 값에 영향을 준다는 아이디어에 대한 모형.\n",
    "현재 시점 = 과거가 현재에 영향을 미치는 영향을 나타내는 모수*시계열 데이터의 과거 시점 + 백색 잡음\n",
    "=> p 시점을 기준으로 그 이전의 데이터에 의해 현재 시점의 데이터가 영향을 받는 모형.\n",
    "\n",
    "7.2.2. MA(Moving Average)\n",
    "트랜드가 변화하는 상황에 적합한 회귀 모델.\n",
    "시계열을 따라 윈도우 크기만큼 슬라이딩하는 이동 평균 모델.\n",
    "현재 시점 = 매개변수*과거 시점의 오차 + 오차 항\n",
    "=> AR 모델과 달리 이전 데이터의 '상태'에서 현재 데이터의 상태를 추론하는 것 x, 이전 데이터의 오차에서 현재 데이터의 상태를 추론.\n",
    "\n",
    "7.2.3. ARMA(AutoRegressive Moving Average)\n",
    "AR + MR 주로 연구 기관에서 사용.\n",
    "AR, MR 두 가지 관점에서 과거의 데이터를 사용.\n",
    "\n",
    "7.2.4. ARIMA(AutoRegressive Integrated Moving Average)\n",
    "자기 회귀와 이동 평균을 둘 다 고려.\n",
    "ARMA와 달리 과거 데이터의 선형 관계뿐만 아니라 추세까지 고려한 모델.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARIMA() 함수를 호출하여 sales 데이터셋에 대한 예측\n",
    "from pandas import read_csv\n",
    "from pandas import datetime\n",
    "from pandas import DataFrame\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from matplotlib import pyplot\n",
    "\n",
    "def parser(x): # 시간을 표현하는 함수 정의\n",
    "    return datetime.strptime('199'+x, '%Y-%m')\n",
    "\n",
    "series = read_csv('../chap7/data/sales.csv', header=0, parse_dates=[0], index_col=0, squeeze=True, date_parser=parser)\n",
    "model = ARIMA(series, order=(5,1,0))\n",
    "model_fit = model.fit(disp=0)\n",
    "print(model_fit.summary())\n",
    "residuals = DataFrame(model_fit.resid)\n",
    "residuals.plot()\n",
    "pyplot.show()\n",
    "residuals.plot(kind='kde')\n",
    "pyplot.show()\n",
    "print(residuals.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pandas import read_csv\n",
    "from pandas import datetime\n",
    "from pandas import DataFrame\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from matplotlib import pyplot\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def parser(x): # 시간을 표현하는 함수 정의\n",
    "    return datetime.strptime('199'+x, '%Y-%m')\n",
    "\n",
    "series = read_csv('../chap7/data/sales.csv', header=0, parse_dates=[0], index_col=0, squeeze=True, date_parser=parser)\n",
    "X = series.values\n",
    "X = np.nan_to_num(X)\n",
    "size = int(len(X)*0.66)\n",
    "train, test = X[0:size], X[size:len(X)]\n",
    "history = [x for x in train]\n",
    "predictions = list()\n",
    "for t in range(len(test)):\n",
    "    model = ARIMA(history, order=(5,1,0))\n",
    "    model_fit = model.fit(disp=0)\n",
    "    output = model_fit.forecast()\n",
    "    yhat = output[0]\n",
    "    predictions.append(yhat)\n",
    "    obs = test[t]\n",
    "    history.append(obs)\n",
    "    print('predicted=%f, expected=%f' % (yhat, obs))\n",
    "error = mean_squared_error(test, predictions)\n",
    "print('Test MSE: %.3f' % error)\n",
    "pyplot.plot(test)\n",
    "pyplot.plot(predictions, color='red')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.3 순환 신경망(RNN)\n",
    "\"\"\"\n",
    "RNN은 시간적으로 연속성이 있는 데이터를 처리하려고 고안된 인공 신경망.\n",
    "'Recurrent(반복되는)는 이전 은닉층이 현재 은닉층의 입력이 되면서 '반복되는 순환 구조를 갖는다'는 의미.\n",
    "RNN이 기존 네트워크와 다른 점은 '기억'을 갖는다는 것.\n",
    "기억 = 현재까지 입력 데이터를 요약한 정보\n",
    "새로운 입력이 네트워크로 들어올 때마다 기억은 조금씩 수정, 결국 최종적으로 남겨진 기억은 모든 입력 전체를 요약한 정보가 됨.\n",
    "\n",
    "입력과 출력의 유형들\n",
    "1. 일대일: 순환이 없음. RNN이라고 하기 어려움. 순방향 네트워크가 대표적.\n",
    "2. 일대다: 입력이 하나이고, 출력이 다수. 예) 이미지 캡셔닝\n",
    "3. 다대일: 입력이 다수이고 출력이 하나. 예) 감성 분석기(문장 -> 긍정/부정)\n",
    "4. 다대다: 입력과 출력이 다수인 구조. 예) 언어 자동 번역기\n",
    "5. 동기화 다대다: 입력과 출력이 다수인 구조. 예) 문장에서 다음에 나올 단어를 예측하는 언어 모델, 프레임 수준의 비디오 분류\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.3.1. RNN 계층과 셀\n",
    "\"\"\"\n",
    "RNN은 내장된 계층뿐만 아니라 셀 레벨의 API도 제공.\n",
    "RNN 계층이 입력된 배치 순서대로 모두 처리하는 것과 다르게 RNN 셀은 오직 하나의 단계만 처리.\n",
    "따라서 RNN 셀은 RNN 계층의 for loop 구문을 갖는 구조.\n",
    "\n",
    "셀 유형\n",
    "1) nn.RNNCell: SimpleRNN 계층에 대응되는 RNN 셀\n",
    "2) nn.GRUCell: GRU 계층에 대응되는 GRU 셀\n",
    "3) nn.LSTMCell: LSTM 계층에 대응되는 LSTM 셀\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.4 RNN 구조\n",
    "\"\"\"\n",
    "RNN에서는 입력층, 은닉층, 출력층 외에 가중치를 세 개 가짐: W_xh, W_hh, W_hy\n",
    "W_xh: 입력층에서 은닉층으로 전달되는 가중치\n",
    "W_hh: t 시점의 은닉층에서 t+1 시점의 은닉층으로 전달되는 가중치\n",
    "W_hy: 은닉층에서 출력층으로 전달되는 가중치\n",
    "가중치 W_xh, W_hh, W_hy는 모든 시점에 동일함.\n",
    "\n",
    "1. 은닉층\n",
    "(이전 은닉층*은닉층 -> 은닉층 가중치 + 입력층 -> 은닉층 가중치*현재 입력 값)\n",
    "일반적으로 하이퍼볼릭 탄젠트 활성화 함수 사용\n",
    "\n",
    "2. 출력층\n",
    "(은닉층 -> 출력층 가중치*현재 은닉층)\n",
    "소프트맥스 함수 적용\n",
    "\n",
    "3. RNN의 오차\n",
    "심층 신경망에서 전방향 학습과 달리 각 단계(t)마다 오차를 측정.\n",
    "즉, 각 단계마다 실제 값과 예측 값으로 오차(평균 제곱 오차(mse) 적용)를 이용하여 측정.\n",
    "\n",
    "4. RNN에서 역전파\n",
    "BPTT(BackPropagation Through Time)를 이용하여 모든 단계마다 처음부터 끝까지 역전파.\n",
    "오차는 각 단계마다 오차를 계산하고 이전 단계로 전달 = BPTT\n",
    "BPTT는 기울기 소멸 문제가 발생. 이를 보완하기 위해 오차를 몇 단계까지만 전파시키는 생략된-BPTT를 사용할 수도 있으나 보통 LSTM, GRU 사용.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.4.1. RNN 셀 구현\n",
    "import torch\n",
    "import torchtext\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리\n",
    "start = time.time()\n",
    "TEXT = torchtext.legacy.data.Field(lower=True, fix_length=200, batch_first=False)\n",
    "LABEL = torchtext.legacy.data.Field(sequential=False)\n",
    "\n",
    "# 데이터셋 준비\n",
    "from torchtext.legacy import datasets\n",
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
    "\n",
    "# 훈련 데이터셋 내용 확인\n",
    "print(vars(train_data.examples[0]))\n",
    "\n",
    "# 데이터 전처리 적용\n",
    "import string\n",
    "for example in train_data.examples:\n",
    "    text = [x.lower() for x in vars(example)['text']] # 소문자로 변경\n",
    "    text = [x.replace(\"<br\", \"\") for x in text] # \"<br\"을 \" \"으로 변경\n",
    "    text = [''.join(c for c in s if c not in string.punctuation) for s in text] # 구두점 제거\n",
    "    text = [s for s in text if s]\n",
    "    vars(example)['text'] = text\n",
    "\n",
    "# 훈련과 검증 데이터셋 분리\n",
    "import random\n",
    "train_data, valid_data = train_data.split(random_state=random.seed(0), split_ratio=0.8)\n",
    "\n",
    "# 단어 집합 만들기\n",
    "TEXT.build_vocab(train_data, max_size=10000, min_freq=10, vectors=None)\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "# 테스트 데이터셋의 단어 집합 확인\n",
    "print(LABEL.vocab.stoi)\n",
    "\n",
    "# 데이터셋 메모리로 가져오기\n",
    "BATCH_SIZE = 64\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "embedding_dim = 100 # 각 단어를 100차원으로 조정(임베딩 계층을 통과한 후 각 벡터의 크기)\n",
    "hidden_size=300\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = torchtext.legacy.data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "일반적으로 계층의 유닛 개수를 늘리는 것보다 계층 자체에 대한 개수를 늘리는 것이 성능을 위해서는 더 좋음.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 워드 임베딩 및 RNN 셀 정의\n",
    "class RNNCell_Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_size):\n",
    "        super(RNNCell_Encoder, self).__init__()\n",
    "        self.rnn = nn.RNNCell(input_dim, hidden_size)\n",
    "    def forward(self, inputs):\n",
    "        bz = inputs.shape[1]\n",
    "        ht = torch.zeros((bz, hidden_size)).to(device) # 배치와 은닉층 뉴런의 크기를 0으로 초기화\n",
    "        for word in inputs:\n",
    "            ht = self.rnn(word, ht)\n",
    "        return ht\n",
    "    \n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.em = nn.Embedding(len(TEXT.vocab.stoi), embedding_dim)\n",
    "        self.rnn = RNNCell_Encoder(embedding_dim, hidden_size)\n",
    "        self.fc1 = nn.Linear(hidden_size, 256)\n",
    "        self.fc2 = nn.Linear(256, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.em(x)\n",
    "        x = self.rnn(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 옵티마이저와 손실 함수 정의\n",
    "model = Net()\n",
    "model.to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.4.2 RNN 계층 구현\n",
    "vocab_size = len(TEXT.vocab)\n",
    "n_classes = 2 # pos, neg\n",
    "\n",
    "# RNN 계층 네트워크\n",
    "class BasicRNN(nn.Module):\n",
    "    def __init__(self, n_layers, hidden_dim, n_vocab, embed_dim, n_classes, dropout_p=0.2):\n",
    "        super(BasicRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.embed = nn.Embedding(n_vocab, embed_dim)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.rnn = nn.RNN(embed_dim, self.hidden_dim, num_layers=self.n_layers, batch_first=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embed(x) # 문자를 숫자/벡터로 변환\n",
    "        h_0 = self._init_state(batch_size=x.size(0))\n",
    "        x, _ = self.rnn(x, h_0)\n",
    "        h_t = x[:, -1, :]\n",
    "        self.dropout(h_t)\n",
    "        logit = torch.sigmoid(self.out(h_t))\n",
    "        return logit\n",
    "    \n",
    "    def _init_state(self, batch_size=1):\n",
    "        weight = next(self.parameters()).data\n",
    "        return weight.new(self.n_layers, batch_size, self.hidden_dim).zero_()\n",
    "    \n",
    "# 손실 함수와 옵티마이저 설정\n",
    "model = BasicRNN(n_layers=1, hidden_dim=256, n_vocab=vocab_size, embed_dim=128, n_classes=n_classes, dropout_p=0.5)\n",
    "model.to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.5 LSTM\n",
    "\"\"\"\n",
    "LSTM은 망각 게이트, 입력 게이트, 출력 게이트라는 새로운 요소를 은닉층의 각 뉴런에 추가.\n",
    "\n",
    "망각 게이트:\n",
    "과거 정보를 어느 정도 기억할지 결정.\n",
    "과거 정보와 현재 데이터를 입력받아 시그모이드를 취한 후 그 값을 과거 정보에 곱함.\n",
    "시그모이드의 출력이 0이면 과거 정보는 버리고, 1이면 온전히 보존.\n",
    "\n",
    "입력 게이트:\n",
    "현재 정보를 기억하기.\n",
    "과거 정보와 현재 데이터 입력받아 시그모이드와 하이퍼볼릭 탄젠트 함수를 기반으로 현재 정보에 대한 보존량 결정.\n",
    "\n",
    "셀:\n",
    "각 단계에 대한 은닉 노드 = 메모리 셀.\n",
    "총합(sum)을 사용하여 셀 값 반영하면 기울기 소멸 문제가 해결됨.\n",
    "\n",
    "출력 게이트:\n",
    "과거 정보와 현재 데이터를 사용해 뉴런의 출력을 결정.\n",
    "계산한 값이 1이면 의미 있는 결과로 최종 출력.\n",
    "0이면 해당 연산 출력을 안 함.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.6 GRU\n",
    "\"\"\"\n",
    "LSTM에서 사용하는 망각 게이트와 입력 게이트를 하나로 합친 것.\n",
    "별도의 업데이트 게이트로 구성.\n",
    "하나의 게이트 컨트롤러가 망각 게이트와 입력 게이트를 모두 제어.\n",
    "게이트 컨트롤러가 1 출력 -> 망각 게이트 열리고 입력 게이트 닫힘.\n",
    "0 출력 -> 망각 게이트 닫히고 입력 게이트 열림.\n",
    "= 이전 기억이 저장될 때마다 단계별 입력은 삭제됨\n",
    "\n",
    "GRU는 출력 게이트가 없어 전체 상태 벡터가 매 단계 출력됨.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
