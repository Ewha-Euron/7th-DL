{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac75932b-c838-40c9-a374-09af81e9c5c4",
   "metadata": {},
   "source": [
    "## 10-1 원핫인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c088ae2a-4e54-43a5-8e42-9039eb897f30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 1, 0, 1, 0])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 원핫인코딩 적용\n",
    "import pandas as pd\n",
    "# 데이터셋을 메모리로 로딩\n",
    "class2=pd.read_csv(\"C:/Users/sshyu/OneDrive/바탕 화면/Euron/chap10/data/class2.csv\")\n",
    "\n",
    "from sklearn import preprocessing \n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "onehot_encoder = preprocessing.OneHotEncoder()\n",
    "# 데이터를 숫자 형식으로 표현\n",
    "train_x = label_encoder.fit_transform(class2['class2'])\n",
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fea96c9c-bc41-43a7-a2c3-c7b88c778425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 13,\n",
       " 'is': 7,\n",
       " 'last': 8,\n",
       " 'chance': 2,\n",
       " 'and': 0,\n",
       " 'if': 6,\n",
       " 'you': 15,\n",
       " 'do': 3,\n",
       " 'not': 10,\n",
       " 'have': 5,\n",
       " 'will': 14,\n",
       " 'never': 9,\n",
       " 'get': 4,\n",
       " 'any': 1,\n",
       " 'one': 11,\n",
       " 'please': 12}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 10.2 횟수 기반 인코딩\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\n",
    "    'This is last chance.',\n",
    "    'and if you do not have this chance.',\n",
    "    'you will never get any chance.',\n",
    "    'will you do get this one?',\n",
    "    'please, get this chance',\n",
    "]\n",
    "vect = CountVectorizer()\n",
    "vect.fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95a515d4-0617-4568-ab4a-e9b5bffbee35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 배열 변환\n",
    "vect.transform(['you will never get any chance.']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "200a36cd-53a9-4b98-92ce-9c5457359a0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'last': 6,\n",
       " 'chance': 1,\n",
       " 'if': 5,\n",
       " 'you': 11,\n",
       " 'do': 2,\n",
       " 'not': 8,\n",
       " 'have': 4,\n",
       " 'will': 10,\n",
       " 'never': 7,\n",
       " 'get': 3,\n",
       " 'any': 0,\n",
       " 'one': 9}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 불용어를 제거한 카운트 벡터\n",
    "vect = CountVectorizer(stop_words=[\"and\", \"is\", \"please\", \"this\"]).fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e60ebd63-0985-47f1-bf20-356e945b8a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "유사도를 위한 3 x 3 matrix를 만들었습니다.\n",
      "[[1.       0.224325 0.      ]\n",
      " [0.224325 1.       0.      ]\n",
      " [0.       0.       1.      ]]\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF를 적용한 후 행렬로 표현\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "doc = ['I like machine learning', 'I love deep learning', 'I run everyday']\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=1)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(doc)\n",
    "doc_distance = (tfidf_matrix * tfidf_matrix.T)\n",
    "print ('유사도를 위한', str(doc_distance.get_shape()[0]), 'x', str(doc_distance.get_shape()[1]), 'matrix를 만들었습니다.')\n",
    "print(doc_distance.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2719572c-8fc4-477e-8436-63b088f2e41d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['once',\n",
       "  'upon',\n",
       "  'a',\n",
       "  'time',\n",
       "  'in',\n",
       "  'london',\n",
       "  ',',\n",
       "  'the',\n",
       "  'darlings',\n",
       "  'went',\n",
       "  'out',\n",
       "  'to',\n",
       "  'a',\n",
       "  'dinner',\n",
       "  'party',\n",
       "  'leaving',\n",
       "  'their',\n",
       "  'three',\n",
       "  'children',\n",
       "  'wendy',\n",
       "  ',',\n",
       "  'jhon',\n",
       "  ',',\n",
       "  'and',\n",
       "  'michael',\n",
       "  'at',\n",
       "  'home',\n",
       "  '.'],\n",
       " ['after',\n",
       "  'wendy',\n",
       "  'had',\n",
       "  'tucked',\n",
       "  'her',\n",
       "  'younger',\n",
       "  'brothers',\n",
       "  'jhon',\n",
       "  'and',\n",
       "  'michael',\n",
       "  'to',\n",
       "  'bed',\n",
       "  ',',\n",
       "  'she',\n",
       "  'went',\n",
       "  'to',\n",
       "  'read',\n",
       "  'a',\n",
       "  'book',\n",
       "  '.'],\n",
       " ['she', 'heard', 'a', 'boy', 'sobbing', 'outside', 'her', 'window', '.'],\n",
       " ['he', 'was', 'flying', '.'],\n",
       " ['there', 'was', 'little', 'fairy', 'fluttering', 'around', 'him', '.'],\n",
       " ['wendy', 'opened', 'the', 'window', 'to', 'talk', 'to', 'him', '.'],\n",
       " ['“', 'hello', '!'],\n",
       " ['who', 'are', 'you', '?'],\n",
       " ['why', 'are', 'you', 'crying', '”', ',', 'wendy', 'asked', 'him', '.'],\n",
       " ['“', 'my', 'name', 'is', 'peter', 'pan', '.'],\n",
       " ['my',\n",
       "  'shadow',\n",
       "  'wouldn',\n",
       "  '’',\n",
       "  't',\n",
       "  'stock',\n",
       "  'to',\n",
       "  'me.',\n",
       "  '”',\n",
       "  ',',\n",
       "  'he',\n",
       "  'replied',\n",
       "  '.'],\n",
       " ['she', 'asked', 'him', 'to', 'come', 'in', '.'],\n",
       " ['peter', 'agreed', 'and', 'came', 'inside', 'the', 'room', '.'],\n",
       " ['wendy',\n",
       "  'took',\n",
       "  'his',\n",
       "  'shadow',\n",
       "  'and',\n",
       "  'sewed',\n",
       "  'it',\n",
       "  'to',\n",
       "  'his',\n",
       "  'shoe',\n",
       "  'tips',\n",
       "  '.'],\n",
       " ['now',\n",
       "  'his',\n",
       "  'shadow',\n",
       "  'followed',\n",
       "  'him',\n",
       "  'wherever',\n",
       "  'peter',\n",
       "  'pan',\n",
       "  'went',\n",
       "  '!'],\n",
       " ['he',\n",
       "  'was',\n",
       "  'delighted',\n",
       "  'and',\n",
       "  'asked',\n",
       "  'wendy',\n",
       "  '“',\n",
       "  'why',\n",
       "  'don',\n",
       "  '’',\n",
       "  't',\n",
       "  'you',\n",
       "  'come',\n",
       "  'with',\n",
       "  'me',\n",
       "  'to',\n",
       "  'my',\n",
       "  'home',\n",
       "  '.'],\n",
       " ['the', 'neverland', '.'],\n",
       " ['i',\n",
       "  'lived',\n",
       "  'there',\n",
       "  'with',\n",
       "  'my',\n",
       "  'fairy',\n",
       "  'tinker',\n",
       "  'bell.',\n",
       "  '”',\n",
       "  'wendy',\n",
       "  '?'],\n",
       " ['“', 'oh', '!'],\n",
       " ['what', 'a', 'wonderful', 'idea', '!'],\n",
       " ['let', 'me', 'wake', 'up', 'john', 'and', 'micheal', 'too', '.'],\n",
       " ['could', 'you', 'teach', 'us', 'how', 'to', 'fly', '?', '”', '.'],\n",
       " ['“', 'yes', '!'],\n",
       " ['of', 'course', '!'],\n",
       " ['get',\n",
       "  'them',\n",
       "  'we',\n",
       "  'will',\n",
       "  'all',\n",
       "  'fly',\n",
       "  'together.',\n",
       "  '”',\n",
       "  'peter',\n",
       "  'pan',\n",
       "  'replied',\n",
       "  'and',\n",
       "  'so',\n",
       "  'it',\n",
       "  'was',\n",
       "  '.'],\n",
       " ['five',\n",
       "  'little',\n",
       "  'figures',\n",
       "  'flew',\n",
       "  'out',\n",
       "  'of',\n",
       "  'the',\n",
       "  'window',\n",
       "  'of',\n",
       "  'the',\n",
       "  'darlings',\n",
       "  'and',\n",
       "  'headed',\n",
       "  'towards',\n",
       "  'neverland',\n",
       "  '.'],\n",
       " ['as',\n",
       "  'they',\n",
       "  'flew',\n",
       "  'over',\n",
       "  'the',\n",
       "  'island',\n",
       "  ',',\n",
       "  'peter',\n",
       "  'pan',\n",
       "  'told',\n",
       "  'the',\n",
       "  'children',\n",
       "  'more',\n",
       "  'about',\n",
       "  'his',\n",
       "  'homeland',\n",
       "  '.'],\n",
       " ['“',\n",
       "  'all',\n",
       "  'the',\n",
       "  'children',\n",
       "  'who',\n",
       "  'get',\n",
       "  'lost',\n",
       "  'come',\n",
       "  'and',\n",
       "  'stay',\n",
       "  'with',\n",
       "  'tinker',\n",
       "  'bell',\n",
       "  'and',\n",
       "  'me',\n",
       "  ',',\n",
       "  '”',\n",
       "  'peter',\n",
       "  'told',\n",
       "  'them',\n",
       "  '.'],\n",
       " ['the', 'indians', 'also', 'live', 'in', 'neverland', '.'],\n",
       " ['the',\n",
       "  'mermaids',\n",
       "  'live',\n",
       "  'in',\n",
       "  'the',\n",
       "  'lagoon',\n",
       "  'around',\n",
       "  'the',\n",
       "  'island',\n",
       "  '.'],\n",
       " ['and',\n",
       "  'a',\n",
       "  'very',\n",
       "  'mean',\n",
       "  'pirate',\n",
       "  'called',\n",
       "  'captain',\n",
       "  'hook',\n",
       "  'keeps',\n",
       "  'troubling',\n",
       "  'everyone',\n",
       "  '.'],\n",
       " ['“', 'crocodile', 'bit', 'his', 'one', 'arm', '.'],\n",
       " ['so',\n",
       "  'the',\n",
       "  'captain',\n",
       "  'had',\n",
       "  'to',\n",
       "  'put',\n",
       "  'a',\n",
       "  'hook',\n",
       "  'in',\n",
       "  'its',\n",
       "  'place',\n",
       "  '.'],\n",
       " ['since', 'then', 'he', 'is', 'afraid', 'of', 'crocodiles', '.'],\n",
       " ['and', 'rightly', 'so', '!'],\n",
       " ['if',\n",
       "  'the',\n",
       "  'crocodile',\n",
       "  'ever',\n",
       "  'found',\n",
       "  'captain',\n",
       "  'hook',\n",
       "  'it',\n",
       "  'will',\n",
       "  'eat',\n",
       "  'up',\n",
       "  'the',\n",
       "  'rest',\n",
       "  'of',\n",
       "  'it',\n",
       "  'couldn',\n",
       "  '’',\n",
       "  't',\n",
       "  'eat',\n",
       "  'last',\n",
       "  'time.',\n",
       "  '”',\n",
       "  'peter',\n",
       "  'told',\n",
       "  'them',\n",
       "  '.'],\n",
       " ['soon', 'they', 'landed', 'on', 'the', 'island', '.'],\n",
       " ['and',\n",
       "  'to',\n",
       "  'the',\n",
       "  'surprise',\n",
       "  'of',\n",
       "  'wendy',\n",
       "  ',',\n",
       "  'jhon',\n",
       "  'and',\n",
       "  'michael',\n",
       "  ',',\n",
       "  'peter',\n",
       "  'pan',\n",
       "  'let',\n",
       "  'them',\n",
       "  'in',\n",
       "  'through',\n",
       "  'a',\n",
       "  'small',\n",
       "  'opening',\n",
       "  'in',\n",
       "  'a',\n",
       "  'tree',\n",
       "  '.'],\n",
       " ['inside',\n",
       "  'the',\n",
       "  'tree',\n",
       "  'was',\n",
       "  'a',\n",
       "  'large',\n",
       "  'room',\n",
       "  'with',\n",
       "  'children',\n",
       "  'inside',\n",
       "  'it',\n",
       "  '.'],\n",
       " ['somewhere',\n",
       "  'huddled',\n",
       "  'by',\n",
       "  'the',\n",
       "  'fire',\n",
       "  'in',\n",
       "  'the',\n",
       "  'corner',\n",
       "  'and',\n",
       "  'somewhere',\n",
       "  'playing',\n",
       "  'amongst',\n",
       "  'themselves',\n",
       "  '.'],\n",
       " ['their',\n",
       "  'faces',\n",
       "  'lit',\n",
       "  'up',\n",
       "  'when',\n",
       "  'they',\n",
       "  'saw',\n",
       "  'peter',\n",
       "  'pan',\n",
       "  ',',\n",
       "  'tinker',\n",
       "  'bell',\n",
       "  ',',\n",
       "  'and',\n",
       "  'their',\n",
       "  'guests',\n",
       "  '.'],\n",
       " ['“', 'hello', 'everyone', '.'],\n",
       " ['this', 'is', 'wendy', ',', 'jhon', ',', 'and', 'michael', '.'],\n",
       " ['they',\n",
       "  'will',\n",
       "  'be',\n",
       "  'staying',\n",
       "  'with',\n",
       "  'us',\n",
       "  'from',\n",
       "  'now',\n",
       "  'on.',\n",
       "  '”',\n",
       "  'peter',\n",
       "  'pan',\n",
       "  'introduced',\n",
       "  'them',\n",
       "  'to',\n",
       "  'all',\n",
       "  'children',\n",
       "  '.'],\n",
       " ['children', 'welcomed', 'wendy', ',', 'jhon', ',', 'and', 'michael', '.'],\n",
       " ['a', 'few', 'days', 'passed', '.'],\n",
       " ['and', 'they', 'settled', 'into', 'a', 'routine', '.'],\n",
       " ['wendy',\n",
       "  'would',\n",
       "  'take',\n",
       "  'care',\n",
       "  'of',\n",
       "  'all',\n",
       "  'the',\n",
       "  'children',\n",
       "  'in',\n",
       "  'the',\n",
       "  'day',\n",
       "  'and',\n",
       "  'would',\n",
       "  'go',\n",
       "  'out',\n",
       "  'with',\n",
       "  'peter',\n",
       "  'pan',\n",
       "  'and',\n",
       "  'her',\n",
       "  'brothers',\n",
       "  'in',\n",
       "  'the',\n",
       "  'evening',\n",
       "  'to',\n",
       "  'learn',\n",
       "  'about',\n",
       "  'the',\n",
       "  'island',\n",
       "  '.'],\n",
       " ['she',\n",
       "  'would',\n",
       "  'cook',\n",
       "  'for',\n",
       "  'them',\n",
       "  'and',\n",
       "  'stitch',\n",
       "  'new',\n",
       "  'clothes',\n",
       "  'for',\n",
       "  'them',\n",
       "  '.'],\n",
       " ['he',\n",
       "  'even',\n",
       "  'made',\n",
       "  'a',\n",
       "  'lovely',\n",
       "  'new',\n",
       "  'dress',\n",
       "  'for',\n",
       "  'tinker',\n",
       "  'bell',\n",
       "  '.'],\n",
       " ['one',\n",
       "  'evening',\n",
       "  ',',\n",
       "  'as',\n",
       "  'they',\n",
       "  'were',\n",
       "  'out',\n",
       "  'exploring',\n",
       "  'the',\n",
       "  'island',\n",
       "  'peter',\n",
       "  'pan',\n",
       "  'warned',\n",
       "  'everyone',\n",
       "  'and',\n",
       "  'said',\n",
       "  ',',\n",
       "  '“',\n",
       "  'hide',\n",
       "  '!'],\n",
       " ['hide', '!'],\n",
       " ['pirates', '!'],\n",
       " ['and',\n",
       "  'they',\n",
       "  'have',\n",
       "  'kidnapped',\n",
       "  'the',\n",
       "  'indian',\n",
       "  'princess',\n",
       "  'tiger',\n",
       "  'lily',\n",
       "  '.'],\n",
       " ['they',\n",
       "  'have',\n",
       "  'kept',\n",
       "  'her',\n",
       "  'there',\n",
       "  ',',\n",
       "  'tied',\n",
       "  'up',\n",
       "  'by',\n",
       "  'the',\n",
       "  'rocks',\n",
       "  ',',\n",
       "  'near',\n",
       "  'the',\n",
       "  'water.',\n",
       "  '”',\n",
       "  'peter',\n",
       "  'was',\n",
       "  'afraid',\n",
       "  'and',\n",
       "  'the',\n",
       "  'princess',\n",
       "  'would',\n",
       "  'drown',\n",
       "  ',',\n",
       "  'is',\n",
       "  'she',\n",
       "  'fell',\n",
       "  'into',\n",
       "  'the',\n",
       "  'water',\n",
       "  '.'],\n",
       " ['so',\n",
       "  ',',\n",
       "  'in',\n",
       "  'a',\n",
       "  'voice',\n",
       "  'that',\n",
       "  'sounded',\n",
       "  'like',\n",
       "  'captain',\n",
       "  'hook',\n",
       "  ',',\n",
       "  'he',\n",
       "  'shouted',\n",
       "  'instructions',\n",
       "  'to',\n",
       "  'the',\n",
       "  'pirates',\n",
       "  'who',\n",
       "  'guarded',\n",
       "  'her',\n",
       "  ',',\n",
       "  '“',\n",
       "  'you',\n",
       "  'fools',\n",
       "  '!'],\n",
       " ['let', 'her', 'go', 'at', 'once', '!'],\n",
       " ['do',\n",
       "  'it',\n",
       "  'before',\n",
       "  'i',\n",
       "  'come',\n",
       "  'there',\n",
       "  'or',\n",
       "  'else',\n",
       "  'i',\n",
       "  'will',\n",
       "  'throw',\n",
       "  'each',\n",
       "  'one',\n",
       "  'of',\n",
       "  'you',\n",
       "  'into',\n",
       "  'the',\n",
       "  'water.',\n",
       "  '”',\n",
       "  'the',\n",
       "  'pirates',\n",
       "  'got',\n",
       "  'scared',\n",
       "  'and',\n",
       "  'immediately',\n",
       "  'released',\n",
       "  'the',\n",
       "  'princes',\n",
       "  '.'],\n",
       " ['she',\n",
       "  'quickly',\n",
       "  'dived',\n",
       "  'into',\n",
       "  'the',\n",
       "  'water',\n",
       "  'and',\n",
       "  'swam',\n",
       "  'to',\n",
       "  'the',\n",
       "  'safety',\n",
       "  'of',\n",
       "  'her',\n",
       "  'home',\n",
       "  '.'],\n",
       " ['soon',\n",
       "  'everyone',\n",
       "  'found',\n",
       "  'out',\n",
       "  'how',\n",
       "  'peter',\n",
       "  'pan',\n",
       "  'had',\n",
       "  'rescued',\n",
       "  'the',\n",
       "  'princess',\n",
       "  '.'],\n",
       " ['when',\n",
       "  'captain',\n",
       "  'hook',\n",
       "  'found',\n",
       "  'out',\n",
       "  'how',\n",
       "  'peter',\n",
       "  'had',\n",
       "  'tricked',\n",
       "  'his',\n",
       "  'men',\n",
       "  'he',\n",
       "  'was',\n",
       "  'furious',\n",
       "  '.'],\n",
       " ['and', 'swore', 'to', 'have', 'his', 'revenge', '.'],\n",
       " ['that',\n",
       "  'night',\n",
       "  'wendy',\n",
       "  'told',\n",
       "  'peter',\n",
       "  'pan',\n",
       "  ',',\n",
       "  'that',\n",
       "  'she',\n",
       "  'and',\n",
       "  'her',\n",
       "  'brother',\n",
       "  'wanted',\n",
       "  'to',\n",
       "  'go',\n",
       "  'back',\n",
       "  'home',\n",
       "  'since',\n",
       "  'they',\n",
       "  'missed',\n",
       "  'their',\n",
       "  'parents',\n",
       "  '.'],\n",
       " ['she',\n",
       "  'said',\n",
       "  'if',\n",
       "  'the',\n",
       "  'lost',\n",
       "  'children',\n",
       "  'could',\n",
       "  'also',\n",
       "  'return',\n",
       "  'to',\n",
       "  'her',\n",
       "  'world',\n",
       "  'they',\n",
       "  'could',\n",
       "  'find',\n",
       "  'a',\n",
       "  'nice',\n",
       "  'home',\n",
       "  'for',\n",
       "  'them',\n",
       "  '.'],\n",
       " ['peter', 'pan', 'didn', '’', 't', 'want', 'to', 'leave', 'neverland', '.'],\n",
       " ['but',\n",
       "  'the',\n",
       "  'sake',\n",
       "  'of',\n",
       "  'the',\n",
       "  'lost',\n",
       "  'children',\n",
       "  'he',\n",
       "  'agreed',\n",
       "  ',',\n",
       "  'although',\n",
       "  'a',\n",
       "  'bit',\n",
       "  'sadly',\n",
       "  '.'],\n",
       " ['he', 'would', 'miss', 'his', 'friends', 'dearly', '.'],\n",
       " ['the',\n",
       "  'next',\n",
       "  'morning',\n",
       "  'all',\n",
       "  'the',\n",
       "  'lost',\n",
       "  'children',\n",
       "  'left',\n",
       "  'with',\n",
       "  'wendy',\n",
       "  ',',\n",
       "  'jhon',\n",
       "  ',',\n",
       "  'and',\n",
       "  'michael',\n",
       "  '.'],\n",
       " ['but',\n",
       "  'on',\n",
       "  'the',\n",
       "  'way',\n",
       "  ',',\n",
       "  'captain',\n",
       "  'hook',\n",
       "  'and',\n",
       "  'his',\n",
       "  'men',\n",
       "  'kidnapped',\n",
       "  'all',\n",
       "  'of',\n",
       "  'them',\n",
       "  '.'],\n",
       " ['he',\n",
       "  'tied',\n",
       "  'them',\n",
       "  'and',\n",
       "  'kept',\n",
       "  'them',\n",
       "  'on',\n",
       "  'once',\n",
       "  'of',\n",
       "  'his',\n",
       "  'ships',\n",
       "  '.'],\n",
       " ['as',\n",
       "  'soon',\n",
       "  'as',\n",
       "  'peter',\n",
       "  'found',\n",
       "  'out',\n",
       "  'about',\n",
       "  'it',\n",
       "  'he',\n",
       "  'rushed',\n",
       "  'to',\n",
       "  'the',\n",
       "  'ship',\n",
       "  '.'],\n",
       " ['he',\n",
       "  'swung',\n",
       "  'himself',\n",
       "  'from',\n",
       "  'a',\n",
       "  'tress',\n",
       "  'branch',\n",
       "  'and',\n",
       "  'on',\n",
       "  'to',\n",
       "  'the',\n",
       "  'deck',\n",
       "  'of',\n",
       "  'the',\n",
       "  'ship',\n",
       "  'where',\n",
       "  'all',\n",
       "  'the',\n",
       "  'children',\n",
       "  'were',\n",
       "  'tied',\n",
       "  'up',\n",
       "  '.'],\n",
       " ['he',\n",
       "  'swung',\n",
       "  'his',\n",
       "  'sword',\n",
       "  'bravely',\n",
       "  'and',\n",
       "  'threw',\n",
       "  'over',\n",
       "  'the',\n",
       "  'pirates',\n",
       "  'who',\n",
       "  'tried',\n",
       "  'to',\n",
       "  'stop',\n",
       "  'him',\n",
       "  '.'],\n",
       " ['quickly',\n",
       "  'he',\n",
       "  'released',\n",
       "  'everyone',\n",
       "  'from',\n",
       "  'their',\n",
       "  'captor',\n",
       "  '’',\n",
       "  's',\n",
       "  'ties',\n",
       "  '.'],\n",
       " ['wendy',\n",
       "  ',',\n",
       "  'jhon',\n",
       "  ',',\n",
       "  'michael',\n",
       "  'and',\n",
       "  'tinker',\n",
       "  'bell',\n",
       "  'helped',\n",
       "  'all',\n",
       "  'the',\n",
       "  'children',\n",
       "  'into',\n",
       "  'the',\n",
       "  'water',\n",
       "  ',',\n",
       "  'where',\n",
       "  'their',\n",
       "  'friends',\n",
       "  'from',\n",
       "  'the',\n",
       "  'indian',\n",
       "  'camp',\n",
       "  'were',\n",
       "  'ready',\n",
       "  'with',\n",
       "  'smaller',\n",
       "  'boats',\n",
       "  'to',\n",
       "  'take',\n",
       "  'them',\n",
       "  'to',\n",
       "  'safety',\n",
       "  'peter',\n",
       "  'pan',\n",
       "  'now',\n",
       "  'went',\n",
       "  'looking',\n",
       "  'for',\n",
       "  'captain',\n",
       "  'hook',\n",
       "  '.'],\n",
       " ['“',\n",
       "  'let',\n",
       "  'us',\n",
       "  'finished',\n",
       "  'this',\n",
       "  'forever',\n",
       "  'mr.',\n",
       "  'hook',\n",
       "  '”',\n",
       "  ',',\n",
       "  'peter',\n",
       "  'challenged',\n",
       "  'captain',\n",
       "  'hook',\n",
       "  '.'],\n",
       " ['“', 'yes', '!'],\n",
       " ['peter',\n",
       "  'pan',\n",
       "  ',',\n",
       "  'you',\n",
       "  'have',\n",
       "  'caused',\n",
       "  'me',\n",
       "  'enough',\n",
       "  'trouble',\n",
       "  '.'],\n",
       " ['it',\n",
       "  'is',\n",
       "  'time',\n",
       "  'that',\n",
       "  'we',\n",
       "  'finished',\n",
       "  'this.',\n",
       "  '”',\n",
       "  'hook',\n",
       "  'replied',\n",
       "  '.'],\n",
       " ['with',\n",
       "  'his',\n",
       "  'sword',\n",
       "  'drawn',\n",
       "  ',',\n",
       "  'he',\n",
       "  'raced',\n",
       "  'towards',\n",
       "  'peter',\n",
       "  'pan',\n",
       "  '.'],\n",
       " ['quick',\n",
       "  'on',\n",
       "  'his',\n",
       "  'feet',\n",
       "  ',',\n",
       "  'peter',\n",
       "  'pan',\n",
       "  'stepped',\n",
       "  'aside',\n",
       "  'and',\n",
       "  'pushed',\n",
       "  'hook',\n",
       "  'inside',\n",
       "  'the',\n",
       "  'sea',\n",
       "  'where',\n",
       "  'the',\n",
       "  'crocodile',\n",
       "  'was',\n",
       "  'waiting',\n",
       "  'to',\n",
       "  'eat',\n",
       "  'the',\n",
       "  'rest',\n",
       "  'of',\n",
       "  'hook',\n",
       "  '.'],\n",
       " ['everyone',\n",
       "  'rejoiced',\n",
       "  'as',\n",
       "  'captain',\n",
       "  'hook',\n",
       "  'was',\n",
       "  'out',\n",
       "  'of',\n",
       "  'their',\n",
       "  'lives',\n",
       "  'forever',\n",
       "  '.'],\n",
       " ['everybody', 'headed', 'back', 'to', 'london', '.'],\n",
       " ['mr.', 'and', 'mrs', '.'],\n",
       " ['darling',\n",
       "  'was',\n",
       "  'so',\n",
       "  'happy',\n",
       "  'to',\n",
       "  'see',\n",
       "  'their',\n",
       "  'children',\n",
       "  'and',\n",
       "  'they',\n",
       "  'agreed',\n",
       "  'to',\n",
       "  'adopt',\n",
       "  'the',\n",
       "  'lost',\n",
       "  'children',\n",
       "  '.'],\n",
       " ['they',\n",
       "  'even',\n",
       "  'asked',\n",
       "  'peter',\n",
       "  'pan',\n",
       "  'to',\n",
       "  'come',\n",
       "  'and',\n",
       "  'live',\n",
       "  'with',\n",
       "  'them',\n",
       "  '.'],\n",
       " ['but',\n",
       "  'peter',\n",
       "  'pan',\n",
       "  'said',\n",
       "  ',',\n",
       "  'he',\n",
       "  'never',\n",
       "  'wanted',\n",
       "  'to',\n",
       "  'grow',\n",
       "  'up',\n",
       "  ',',\n",
       "  'so',\n",
       "  'he',\n",
       "  'and',\n",
       "  'tinker',\n",
       "  'bell',\n",
       "  'will',\n",
       "  'go',\n",
       "  'back',\n",
       "  'to',\n",
       "  'neverland',\n",
       "  '.'],\n",
       " ['peter',\n",
       "  'pan',\n",
       "  'promised',\n",
       "  'everyone',\n",
       "  'that',\n",
       "  'he',\n",
       "  'will',\n",
       "  'visit',\n",
       "  'again',\n",
       "  'sometime',\n",
       "  '!'],\n",
       " ['and',\n",
       "  'he',\n",
       "  'flew',\n",
       "  'out',\n",
       "  'of',\n",
       "  'the',\n",
       "  'window',\n",
       "  'with',\n",
       "  'tinker',\n",
       "  'bell',\n",
       "  'by',\n",
       "  'his',\n",
       "  'side',\n",
       "  '.']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터셋을 메모리로 로딩하고 토큰화 적용\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "import warnings \n",
    "warnings.filterwarnings(action = 'ignore') \n",
    "import gensim \n",
    "from gensim.models import Word2Vec\n",
    "  \n",
    "sample = open(\"C:/Users/sshyu/OneDrive/바탕 화면/Euron/chap10/data/peter.txt\", \"r\", encoding='UTF8')\n",
    "s = sample.read() \n",
    "  \n",
    "f = s.replace(\"\\n\", \" \")\n",
    "data = [] \n",
    "  \n",
    "for i in sent_tokenize(f):\n",
    "    temp = [] \n",
    "    for j in word_tokenize(i):\n",
    "        temp.append(j.lower())\n",
    "    data.append(temp) \n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6861ba5-6fb5-471a-9c14-315f9e91f886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'peter' wendy' - CBOW :  1.0\n"
     ]
    }
   ],
   "source": [
    "# CBOW 적용 후 peter과 wendy의 유사성 확인\n",
    "model1 = gensim.models.Word2Vec(data, min_count = 1,  \n",
    "                                # 이때 min_count는 단어에 대한 최소 빈도수 제한\n",
    "                              vector_size = 100, window = 5, sg=0)\n",
    "# vector size는 워드 벡터의 특징 값으로 임베딩된 벡터의 차원\n",
    "# window는 window의 context 크기\n",
    "# sg=0일때는 CBOW를 의미, 1일 때는 skip-gram을 의미함\n",
    "print(\"Cosine similarity between 'peter' \" +\n",
    "                 \"wendy' - CBOW : \", \n",
    "      model1.wv.similarity('wendy', 'wendy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd4cdb36-9ca2-4090-b88b-f00046716767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'peter' hook' - CBOW :  0.027709864\n"
     ]
    }
   ],
   "source": [
    "# peter과 hook의 유사성 확인\n",
    "print(\"Cosine similarity between 'peter' \" +\n",
    "                 \"hook' - CBOW : \", \n",
    "      model1.wv.similarity('peter', 'hook')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "419cb3e9-b861-4fcc-9200-56ef36533a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'peter' wendy' - Skip Gram :  0.40088683\n"
     ]
    }
   ],
   "source": [
    "# skip-gram 적용 후 peter과 wendy의 유사성 확인\n",
    "model2 = gensim.models.Word2Vec(data, min_count = 1, vector_size = 100, \n",
    "                                             window = 5, sg = 1)\n",
    "print(\"Cosine similarity between 'peter' \" +\n",
    "          \"wendy' - Skip Gram : \", \n",
    "    model2.wv.similarity('peter', 'wendy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5146da6d-4937-45d3-94cd-dc8baebdcd35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'peter' hook' - Skip Gram :  0.52016735\n"
     ]
    }
   ],
   "source": [
    "# peter과 hook의 유사성 확인\n",
    "print(\"Cosine similarity between 'peter' \" +\n",
    "            \"hook' - Skip Gram : \", \n",
    "      model2.wv.similarity('peter', 'hook')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5eae28e7-52b6-4678-aeb1-2a3323314a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FastText\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import FastText\n",
    "\n",
    "model = FastText(\"C:/Users/sshyu/OneDrive/바탕 화면/Euron/chap10/data/peter.txt\", vector_size=4, window=3, min_count=1, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c72aaedd-dd47-4ffb-bb28-49e9d15cd56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4592452\n"
     ]
    }
   ],
   "source": [
    "# peter과 wendy에 대한 코사인 유사도 확인\n",
    "sim_score = model.wv.similarity('peter', 'wendy')\n",
    "print(sim_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc74e46d-281d-4516-90bf-4506de898116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.043825716\n"
     ]
    }
   ],
   "source": [
    "# peter과 hook에 대한 코사인 유사도 확인\n",
    "sim_score = model.wv.similarity('peter', 'hook')\n",
    "print(sim_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "edab8985-41b2-4c02-9637-2990487d535e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전 훈련된 예제를 내려받기\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model_kr = KeyedVectors.load_word2vec_format(\"C:/Users/sshyu/Downloads/wiki.ko.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e4b68456-b41c-4085-9f7c-08367acbc08c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: 노력함, Similarity: 0.80\n",
      "Word: 노력중, Similarity: 0.75\n",
      "Word: 노력만, Similarity: 0.72\n",
      "Word: 노력과, Similarity: 0.71\n",
      "Word: 노력의, Similarity: 0.69\n",
      "Word: 노력가, Similarity: 0.69\n",
      "Word: 노력이나, Similarity: 0.69\n",
      "Word: 노력없이, Similarity: 0.68\n",
      "Word: 노력맨, Similarity: 0.68\n",
      "Word: 노력보다는, Similarity: 0.68\n"
     ]
    }
   ],
   "source": [
    "# 노련과 유사한 단어의 유사도 확인\n",
    "find_similar_to = '노력'\n",
    "\n",
    "for similar_word in model_kr.similar_by_word(find_similar_to):\n",
    "    print(\"Word: {0}, Similarity: {1:.2f}\".format(\n",
    "        similar_word[0], similar_word[1]\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41519758-0daf-4922-a4b4-3445225c787d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('초식동물', 0.7804121971130371), ('거대동물', 0.7547271251678467), ('육식동물의', 0.7547166347503662), ('유두동물', 0.753511369228363), ('반추동물', 0.7470759749412537), ('독동물', 0.7466291189193726), ('육상동물', 0.7460315823554993), ('유즐동물', 0.7450903654098511), ('극피동물', 0.7449345588684082), ('복모동물', 0.7424345016479492)]\n"
     ]
    }
   ],
   "source": [
    "# 동물, 육식동물에는 긍정적이지만 사람에는 부정적인 단어와 유사도 확인\n",
    "similarities = model_kr.most_similar(positive=['동물', '육식동물'], negative=['사람'])\n",
    "print(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae0984a8-fe63-45bc-b817-c4f53c8f7efd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 100)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GloVe\n",
    "import numpy as np\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "from sklearn.decomposition import PCA\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "glove_file = datapath(\"C:/Users/sshyu/OneDrive/바탕 화면/Euron/chap10/data/glove.6B.100d.txt\")\n",
    "# 글로브 데이터를 워드투벡터 형태로 변환\n",
    "word2vec_glove_file = get_tmpfile(\"glove.6B.100d.word2vec.txt\")\n",
    "glove2word2vec(glove_file, word2vec_glove_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "19849180-4e06-461b-b69a-a5b6485aaeee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('legislation', 0.8072139620780945),\n",
       " ('proposal', 0.7306863069534302),\n",
       " ('senate', 0.7142541408538818),\n",
       " ('bills', 0.704440176486969),\n",
       " ('measure', 0.6958034634590149),\n",
       " ('passed', 0.690624475479126),\n",
       " ('amendment', 0.6846878528594971),\n",
       " ('provision', 0.684556782245636),\n",
       " ('plan', 0.6816463470458984),\n",
       " ('clinton', 0.6663140058517456)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bill과 유사한 단어의 리스트 반환\n",
    "model = KeyedVectors.load_word2vec_format(word2vec_glove_file)\n",
    "model.most_similar('bill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f65901a-df27-4b44-b586-16a141768563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('peach', 0.688809871673584),\n",
       " ('mango', 0.6838189959526062),\n",
       " ('plum', 0.6684104204177856),\n",
       " ('berry', 0.6590359210968018),\n",
       " ('grove', 0.658155083656311),\n",
       " ('blossom', 0.6503506898880005),\n",
       " ('raspberry', 0.6477391123771667),\n",
       " ('strawberry', 0.6442098021507263),\n",
       " ('pine', 0.6390929222106934),\n",
       " ('almond', 0.6379212141036987)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cherry와 유사한 단어의 리스트 반환\n",
    "model.most_similar('cherry') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fe73ec59-d80c-41e0-a91f-9dae67855ffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kazushige', 0.4834350645542145),\n",
       " ('askerov', 0.4778185784816742),\n",
       " ('lakpa', 0.4691525399684906),\n",
       " ('ex-gay', 0.45713329315185547),\n",
       " ('tadayoshi', 0.4522107243537903),\n",
       " ('turani', 0.44810065627098083),\n",
       " ('saglam', 0.446959912776947),\n",
       " ('aijun', 0.443526953458786),\n",
       " ('adjustors', 0.44235292077064514),\n",
       " ('nyum', 0.4423118233680725)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cherry와 관련성이 없는 단어의 리스트 반환\n",
    "model.most_similar(negative='cherry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d66034b-ee83-4be8-8214-55900a871dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queen: 0.7699\n"
     ]
    }
   ],
   "source": [
    "# woman, king과 유사성이 높음 + man과 관련성이 없는 단어를 반환\n",
    "result = model.most_similar(positive=['woman', 'king'], negative=['man'])\n",
    "print(\"{}: {:.4f}\".format(*result[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "73e4b6da-667e-468b-8745-217810a715a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'champagne'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# austrlia, beer, france와 관련성이 있는 단어를 반환\n",
    "def analogy(x1, x2, y1):\n",
    "    result = model.most_similar(positive=[y1, x2], negative=[x1])\n",
    "    return result[0][0]\n",
    "analogy('australia', 'beer', 'france')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bc1112c3-1d52-4ecd-9384-f2f4d6aa66d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'longest'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tall, tallest, long단어를 기반으로 새로운 단어를 유추\n",
    "analogy('tall', 'tallest', 'long')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e8815849-6cc1-4591-8726-3075b2793b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cereal\n"
     ]
    }
   ],
   "source": [
    "# breakfast cereal dinner lunch 중 유사도가 낮은 단어를 반환\n",
    "print(model.doesnt_match(\"breakfast cereal dinner lunch\".split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "700bd1dc-5f1a-48d4-af18-3ffff8ef0381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq2seq\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9dc56592-7eba-4f2c-b08b-e93e3943d99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 셋 준비\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "MAX_LENGTH = 20\n",
    "\n",
    "# 단어를 만들기 위한 클래스\n",
    "class Lang:\n",
    "    def __init__(self):\n",
    "        # 단어의 인덱스를 저장하기 위한 컨테이너를 초기화\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        # SOS 문장의 시작\n",
    "        # EOS 문장의 끝\n",
    "        self.n_words = 2  \n",
    "\n",
    "    # 문장을 단어 단위로 분리한 후 word에 추가\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "    # word에 단어가 없으면 추가되고, 있다면 카운트를 업데이트\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "31105894-fc8a-4705-9651-effe12d3920c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 정규화\n",
    "def normalizeString(df, lang):\n",
    "    sentence = df[lang].str.lower() # 소문자로 전환\n",
    "    sentence = sentence.str.replace('[^A-Za-z\\s]+', '') # a-z, A-Z, ?!를 제외하곤 모두 공백으로\n",
    "    sentence = sentence.str.normalize('NFD') # 유니코드 정규화 방식\n",
    "    # 아스키 코드로 전환\n",
    "    sentence = sentence.str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "    return sentence\n",
    "\n",
    "def read_sentence(df, lang1, lang2):\n",
    "    # 데이터셋의 첫번째 열 - 영어\n",
    "    sentence1 = normalizeString(df, lang1)\n",
    "    # 데이터셋의 두번째 열 - 프랑스어\n",
    "    sentence2 = normalizeString(df, lang2)\n",
    "    return sentence1, sentence2\n",
    "\n",
    "def read_file(loc, lang1, lang2):\n",
    "    df = pd.read_csv(loc, delimiter='\\t', header=None, names=[lang1, lang2])\n",
    "    return df\n",
    "\n",
    "def process_data(lang1,lang2):\n",
    "    # 데이터 불러오기\n",
    "    df = read_file('C:/Users/sshyu/OneDrive/바탕 화면/Euron/chap10/data/%s-%s.txt' % (lang1, lang2), lang1, lang2)\n",
    "    sentence1, sentence2 = read_sentence(df, lang1, lang2)\n",
    "\n",
    "    input_lang = Lang()\n",
    "    output_lang = Lang()\n",
    "    pairs = []\n",
    "    for i in range(len(df)):\n",
    "        if len(sentence1[i].split(' ')) < MAX_LENGTH and len(sentence2[i].split(' ')) < MAX_LENGTH:\n",
    "            # 첫번째와 두번째열을 합쳐서 저장\n",
    "            full = [sentence1[i], sentence2[i]]\n",
    "            # 입력을 영어로\n",
    "            input_lang.addSentence(sentence1[i])\n",
    "            # 출력을 프랑스어로\n",
    "            output_lang.addSentence(sentence2[i])\n",
    "            # pairs에는 입력과 출력이 합쳐진 것으로\n",
    "            pairs.append(full)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "75eed29b-716b-4f69-ae81-884418673925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텐서로 변환\n",
    "# 문장을 단어로 분리하고 인덱스 반환\n",
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "# 딕셔너리에 단어에 대한 인덱스를 가져오고 문장 끝에 토큰을 추가\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "# 입력과 출력 문장을 텐서로 변환\n",
    "def tensorsFromPair(input_lang, output_lang, pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6c2794e5-504c-4c43-816c-5df9723fe538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더 네트워크\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, embbed_dim, num_layers):\n",
    "        super(Encoder, self).__init__()       \n",
    "        self.input_dim = input_dim\n",
    "        self.embbed_dim = embbed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(input_dim, self.embbed_dim)\n",
    "        self.gru = nn.GRU(self.embbed_dim, self.hidden_dim, num_layers=self.num_layers)\n",
    "              \n",
    "    def forward(self, src):      \n",
    "        embedded = self.embedding(src).view(1,1,-1)\n",
    "        outputs, hidden = self.gru(embedded)\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "19c6113f-238b-4c2e-9a02-b34d4bf1676e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더 네트워크\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, hidden_dim, embbed_dim, num_layers):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.embbed_dim = embbed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(output_dim, self.embbed_dim)\n",
    "        self.gru = nn.GRU(self.embbed_dim, self.hidden_dim, num_layers=self.num_layers)\n",
    "        self.out = nn.Linear(self.hidden_dim, output_dim)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "      \n",
    "    def forward(self, input, hidden):\n",
    "        input = input.view(1, -1)\n",
    "        embedded = F.relu(self.embedding(input))\n",
    "        output, hidden = self.gru(embedded, hidden)       \n",
    "        prediction = self.softmax(self.out(output[0]))      \n",
    "        return prediction, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "774d2ee5-6e61-4957-9974-6a3c6538dad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq2seq 네트워크\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device, MAX_LENGTH=MAX_LENGTH):\n",
    "        super().__init__()\n",
    "      \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "     \n",
    "    def forward(self, input_lang, output_lang, teacher_forcing_ratio=0.5):\n",
    "\n",
    "        input_length = input_lang.size(0)\n",
    "        batch_size = output_lang.shape[1] \n",
    "        target_length = output_lang.shape[0]\n",
    "        vocab_size = self.decoder.output_dim      \n",
    "        outputs = torch.zeros(target_length, batch_size, vocab_size).to(self.device)\n",
    "\n",
    "        for i in range(input_length):\n",
    "            encoder_output, encoder_hidden = self.encoder(input_lang[i])\n",
    "\n",
    "        decoder_hidden = encoder_hidden.to(device)  \n",
    "        decoder_input = torch.tensor([SOS_token], device=device)  \n",
    "\n",
    "        for t in range(target_length):   \n",
    "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
    "            outputs[t] = decoder_output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            input = (output_lang[t] if teacher_force else topi)\n",
    "            if(teacher_force == False and input.item() == EOS_token):\n",
    "                break\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "66728f6c-ba87-462a-a00b-971a33b9bde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델의 오차 계산 함수 정의\n",
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "def Model(model, input_tensor, target_tensor, model_optimizer, criterion):\n",
    "    model_optimizer.zero_grad()\n",
    "    input_length = input_tensor.size(0)\n",
    "    loss = 0\n",
    "    epoch_loss = 0\n",
    "    output = model(input_tensor, target_tensor)\n",
    "    num_iter = output.size(0)\n",
    "\n",
    "    for ot in range(num_iter):\n",
    "        loss += criterion(output[ot], target_tensor[ot])\n",
    "\n",
    "    loss.backward()\n",
    "    model_optimizer.step()\n",
    "    epoch_loss = loss.item() / num_iter\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fc7e9bec-0bb7-4368-a29c-dc7ab882c8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 훈련 함수 정의\n",
    "def trainModel(model, input_lang, output_lang, pairs, num_iteration=20000):\n",
    "    model.train()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "    criterion = nn.NLLLoss()\n",
    "    total_loss_iterations = 0\n",
    "\n",
    "    training_pairs = [tensorsFromPair(input_lang, output_lang, random.choice(pairs))\n",
    "                      for i in range(num_iteration)]\n",
    "  \n",
    "    for iter in range(1, num_iteration+1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "        loss = Model(model, input_tensor, target_tensor, optimizer, criterion)\n",
    "        total_loss_iterations += loss\n",
    "\n",
    "        if iter % 5000 == 0:\n",
    "            average_loss= total_loss_iterations / 5000\n",
    "            total_loss_iterations = 0\n",
    "            print('%d %.4f' % (iter, average_loss))\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bd41abbc-d621-4142-8e87-c847652fd339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 평가 함수 정의\n",
    "def evaluate(model, input_lang, output_lang, sentences, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentences[0])\n",
    "        output_tensor = tensorFromSentence(output_lang, sentences[1])  \n",
    "        decoded_words = []  \n",
    "        output = model(input_tensor, output_tensor)\n",
    "  \n",
    "        for ot in range(output.size(0)):\n",
    "            topv, topi = output[ot].topk(1)\n",
    "\n",
    "            if topi[0].item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi[0].item()])\n",
    "    return decoded_words\n",
    "\n",
    "def evaluateRandomly(model, input_lang, output_lang, pairs, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('input {}'.format(pair[0]))\n",
    "        print('output {}'.format(pair[1]))\n",
    "        output_words = evaluate(model, input_lang, output_lang, pair)\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('predicted {}'.format(output_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "996e5338-a63e-44b1-b514-2a3bd08a88c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random sentence ['he participated in the debate.', 'il a participe aux debats.']\n",
      "Input : 23191 Output : 39387\n",
      "Encoder(\n",
      "  (embedding): Embedding(23191, 256)\n",
      "  (gru): GRU(256, 512)\n",
      ")\n",
      "Decoder(\n",
      "  (embedding): Embedding(39387, 256)\n",
      "  (gru): GRU(256, 512)\n",
      "  (out): Linear(in_features=512, out_features=39387, bias=True)\n",
      "  (softmax): LogSoftmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 모델 훈련시키기\n",
    "lang1 = 'eng'\n",
    "lang2 = 'fra'\n",
    "input_lang, output_lang, pairs = process_data(lang1, lang2)\n",
    "\n",
    "randomize = random.choice(pairs)\n",
    "print('random sentence {}'.format(randomize))\n",
    "\n",
    "input_size = input_lang.n_words\n",
    "output_size = output_lang.n_words\n",
    "print('Input : {} Output : {}'.format(input_size, output_size))\n",
    "\n",
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "num_layers = 1\n",
    "num_iteration = 50\n",
    "\n",
    "encoder = Encoder(input_size, hidden_size, embed_size, num_layers)\n",
    "decoder = Decoder(output_size, hidden_size, embed_size, num_layers)\n",
    "\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)\n",
    " \n",
    "print(encoder)\n",
    "print(decoder)\n",
    "\n",
    "model = trainModel(model, input_lang, output_lang, pairs, num_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "41895652-403d-4535-bcd8-099c0be1a68e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input so what happened?\n",
      "output alors, que s'est-il passe ?\n",
      "predicted je je je je je je\n",
      "input i don't believe i caught your name.\n",
      "output je ne crois pas avoir retenu ton nom.\n",
      "predicted je je je je je je je <EOS>\n",
      "input did they live here?\n",
      "output vivaient-ils ici ?\n",
      "predicted je je je je\n",
      "input it really depends on when.\n",
      "output ca va dependre de quand.\n",
      "predicted je je je je je je\n",
      "input he knows nothing about politics.\n",
      "output il ignore tout de la politique.\n",
      "predicted je je je je je je je\n",
      "input i was a little put out by this.\n",
      "output j'en fus un peu irrite.\n",
      "predicted je je je je je je\n",
      "input tom is not as fat as i am.\n",
      "output tom n'est pas aussi gros que moi.\n",
      "predicted je je je je je je je <EOS>\n",
      "input i have the complete works of shakespeare.\n",
      "output je possede les uvres completes de shakespeare.\n",
      "predicted je je je je je je je <EOS>\n",
      "input this is all very disturbing.\n",
      "output tout ceci est tres perturbant.\n",
      "predicted je je je je je je\n",
      "input i can't tell you yet.\n",
      "output je ne peux vous le dire encore.\n",
      "predicted je je je je je je je <EOS>\n"
     ]
    }
   ],
   "source": [
    "# 임의의 문장에 대한 평가 확인하기\n",
    "evaluateRandomly(model, input_lang, output_lang, pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ff774a14-7fe8-414e-a9f5-2da53acf0165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어텐션이 적용된 디코더\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a9692843-88b4-410b-bd44-08fccc34a1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어탠션 디코더 모델 학습을 위한 함수\n",
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  \n",
    "    plot_loss_total = 0  \n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensorsFromPair(input_lang, output_lang, random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "        loss = Model(model, input_tensor, target_tensor, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % 5000 == 0:\n",
    "            print_loss_avg = print_loss_total / 5000\n",
    "            print_loss_total = 0\n",
    "            print('%d,  %.4f' % (iter, print_loss_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7b5d9f9c-1793-44dd-9f0c-b826a2aedeeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder(\n",
      "  (embedding): Embedding(23191, 256)\n",
      "  (gru): GRU(256, 512)\n",
      ")\n",
      "AttnDecoderRNN(\n",
      "  (embedding): Embedding(39387, 512)\n",
      "  (attn): Linear(in_features=1024, out_features=20, bias=True)\n",
      "  (attn_combine): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (gru): GRU(512, 512)\n",
      "  (out): Linear(in_features=512, out_features=39387, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 어탠션 디코더 모델 훈련\n",
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "num_layers = 1\n",
    "input_size = input_lang.n_words\n",
    "output_size = output_lang.n_words\n",
    "\n",
    "encoder1 = Encoder(input_size, hidden_size, embed_size, num_layers)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, output_size, dropout_p=0.1).to(device)\n",
    "\n",
    "print(encoder1)\n",
    "print(attn_decoder1)\n",
    "\n",
    "attn_model = trainIters(encoder1, attn_decoder1, 50, print_every=10, plot_every=100, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6df1e765-86d1-4e45-9ca9-4483ce511424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.47.1-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\sshyu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (3.16.1)\n",
      "Collecting huggingface-hub<1.0,>=0.24.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.27.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\sshyu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sshyu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\sshyu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\sshyu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\sshyu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.5-cp312-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\sshyu\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (4.66.6)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\sshyu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\sshyu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\sshyu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sshyu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sshyu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sshyu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sshyu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2024.7.4)\n",
      "Downloading transformers-4.47.1-py3-none-any.whl (10.1 MB)\n",
      "   ---------------------------------------- 0.0/10.1 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 2.4/10.1 MB 11.2 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 5.0/10.1 MB 11.6 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 7.6/10.1 MB 11.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.7/10.1 MB 11.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.1/10.1 MB 10.4 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.27.0-py3-none-any.whl (450 kB)\n",
      "Downloading safetensors-0.4.5-cp312-none-win_amd64.whl (286 kB)\n",
      "Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ----------------------------------- ---- 2.1/2.4 MB 10.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 6.8 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.27.0 safetensors-0.4.5 tokenizers-0.21.0 transformers-4.47.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-transformers\n",
      "  Downloading pytorch_transformers-1.2.0-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: torch>=1.0.0 in c:\\users\\sshyu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pytorch-transformers) (2.4.0+cu124)\n",
      "Requirement already satisfied: numpy in c:\\users\\sshyu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pytorch-transformers) (1.26.4)\n",
      "Collecting boto3 (from pytorch-transformers)\n",
      "  Downloading boto3-1.35.90-py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\sshyu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pytorch-transformers) (2.32.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\sshyu\\appdata\\roaming\\python\\python312\\site-packages (from pytorch-transformers) (4.66.6)\n",
      "Requirement already satisfied: regex in c:\\users\\sshyu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pytorch-transformers) (2024.11.6)\n",
      "Collecting sentencepiece (from pytorch-transformers)\n",
      "  Downloading sentencepiece-0.2.0-cp312-cp312-win_amd64.whl.metadata (8.3 kB)\n",
      "Collecting sacremoses (from pytorch-transformers)\n",
      "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\sshyu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.0.0->pytorch-transformers) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\sshyu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.0.0->pytorch-transformers) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\sshyu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.0.0->pytorch-transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\sshyu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.0.0->pytorch-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sshyu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.0.0->pytorch-transformers) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\sshyu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.0.0->pytorch-transformers) (2024.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sshyu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.0.0->pytorch-transformers) (72.2.0)\n",
      "Collecting botocore<1.36.0,>=1.35.90 (from boto3->pytorch-transformers)\n",
      "  Downloading botocore-1.35.90-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3->pytorch-transformers)\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3->pytorch-transformers)\n",
      "  Downloading s3transfer-0.10.4-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sshyu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->pytorch-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sshyu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->pytorch-transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sshyu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->pytorch-transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sshyu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->pytorch-transformers) (2024.7.4)\n",
      "Requirement already satisfied: click in c:\\users\\sshyu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sacremoses->pytorch-transformers) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\sshyu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sacremoses->pytorch-transformers) (1.4.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\sshyu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm->pytorch-transformers) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\sshyu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from botocore<1.36.0,>=1.35.90->boto3->pytorch-transformers) (2.9.0.post0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sshyu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch>=1.0.0->pytorch-transformers) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\sshyu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch>=1.0.0->pytorch-transformers) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sshyu\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.36.0,>=1.35.90->boto3->pytorch-transformers) (1.16.0)\n",
      "Downloading pytorch_transformers-1.2.0-py3-none-any.whl (176 kB)\n",
      "Downloading boto3-1.35.90-py3-none-any.whl (139 kB)\n",
      "Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
      "   ---------------------------------------- 0.0/897.5 kB ? eta -:--:--\n",
      "   --------------------------------------- 897.5/897.5 kB 10.1 MB/s eta 0:00:00\n",
      "Downloading sentencepiece-0.2.0-cp312-cp312-win_amd64.whl (991 kB)\n",
      "   ---------------------------------------- 0.0/992.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 992.0/992.0 kB 7.8 MB/s eta 0:00:00\n",
      "Downloading botocore-1.35.90-py3-none-any.whl (13.3 MB)\n",
      "   ---------------------------------------- 0.0/13.3 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 2.4/13.3 MB 12.2 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 4.7/13.3 MB 11.9 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 7.3/13.3 MB 11.6 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 9.4/13.3 MB 11.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 11.8/13.3 MB 11.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 13.3/13.3 MB 10.8 MB/s eta 0:00:00\n",
      "Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Downloading s3transfer-0.10.4-py3-none-any.whl (83 kB)\n",
      "Installing collected packages: sentencepiece, jmespath, sacremoses, botocore, s3transfer, boto3, pytorch-transformers\n",
      "Successfully installed boto3-1.35.90 botocore-1.35.90 jmespath-1.0.1 pytorch-transformers-1.2.0 s3transfer-0.10.4 sacremoses-0.1.1 sentencepiece-0.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# BERT\n",
    "!pip install transformers\n",
    "!pip install pytorch-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "82fff7da-252a-4dd7-b45c-22beb3e9a442",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from pytorch_transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "89d9aa29-f955-4e8e-85b9-31209c4d546c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 불러오기\n",
    "train_df = pd.read_csv(\"C:/Users/sshyu/OneDrive/바탕 화면/Euron/chap10/data/training.txt\", sep='\\t')\n",
    "valid_df = pd.read_csv(\"C:/Users/sshyu/OneDrive/바탕 화면/Euron/chap10/data/validing.txt\", sep='\\t')\n",
    "test_df = pd.read_csv(\"C:/Users/sshyu/OneDrive/바탕 화면/Euron/chap10/data/testing.txt\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5e31da39-5d4d-4998-9265-aefb225f3728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 중 일부만 사용\n",
    "train_df = train_df.sample(frac=0.1, random_state=500)\n",
    "valid_df = valid_df.sample(frac=0.1, random_state=500)\n",
    "test_df = test_df.sample(frac=0.1, random_state=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ed900681-5fd9-40d8-825d-18c844e00469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 생성\n",
    "class Datasets(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.df.iloc[idx, 1]\n",
    "        label = self.df.iloc[idx, 2]\n",
    "        return text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a57847a5-4f39-46d1-9339-896aeaa7d5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 데이터셋 데이터 로더로 전달\n",
    "train_dataset = Datasets(train_df)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=0)\n",
    "\n",
    "valid_dataset = Datasets(valid_df)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=2, shuffle=True, num_workers=0)\n",
    "\n",
    "test_dataset = Datasets(test_df)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3aaf1239-f587-4c71-bf4b-296c2e987d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 231508/231508 [00:00<00:00, 436296.69B/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████| 433/433 [00:00<?, ?B/s]\n",
      "100%|████████████████████████████████████████████████████████████████| 440473133/440473133 [01:13<00:00, 5992310.17B/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BERT 토크나이저 내려받기\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6836e3ef-aebd-446d-9df9-fa04a601420c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최적화 모델 확인\n",
    "# 모델 평가를 위해 훈련 과정 저장\n",
    "def save_checkpoint(save_path, model, valid_loss):\n",
    "    if save_path == None:\n",
    "        return    \n",
    "    state_dict = {'model_state_dict': model.state_dict(),\n",
    "                  'valid_loss': valid_loss}\n",
    "    \n",
    "    torch.save(state_dict, save_path)\n",
    "    print(f'Model saved to ==> {save_path}')\n",
    "# save checkpoint함수에서 저장된 모델 가져옴\n",
    "def load_checkpoint(load_path, model):    \n",
    "    if load_path==None:\n",
    "        return    \n",
    "    state_dict = torch.load(load_path, map_location=device)\n",
    "    print(f'Model loaded from <== {load_path}')\n",
    "    \n",
    "    model.load_state_dict(state_dict['model_state_dict'])\n",
    "    return state_dict['valid_loss']\n",
    "# 훈련, 검증에 대한 오차와 에포크를 저장\n",
    "def save_metrics(save_path, train_loss_list, valid_loss_list, global_steps_list):\n",
    "    if save_path == None:\n",
    "        return    \n",
    "    state_dict = {'train_loss_list': train_loss_list,\n",
    "                  'valid_loss_list': valid_loss_list,\n",
    "                  'global_steps_list': global_steps_list}    \n",
    "    torch.save(state_dict, save_path)\n",
    "    print(f'Model saved to ==> {save_path}')\n",
    "# 저장해둔 정보를 불러옴\n",
    "def load_metrics(load_path):\n",
    "    if load_path==None:\n",
    "        return    \n",
    "    state_dict = torch.load(load_path, map_location=device)\n",
    "    print(f'Model loaded from <== {load_path}')    \n",
    "    return state_dict['train_loss_list'], state_dict['valid_loss_list'], state_dict['global_steps_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f6199cb9-f5eb-4d39-9ba0-0f1eef22cdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 훈련 함수 정의\n",
    "def train(model,\n",
    "          optimizer,\n",
    "          criterion = nn.BCELoss(),\n",
    "          num_epochs = 5,\n",
    "          eval_every = len(train_loader) // 2,\n",
    "          best_valid_loss = float(\"Inf\")):\n",
    "    \n",
    "    total_correct = 0.0\n",
    "    total_len = 0.0\n",
    "    running_loss = 0.0\n",
    "    valid_running_loss = 0.0\n",
    "    global_step = 0\n",
    "    train_loss_list = []\n",
    "    valid_loss_list = []\n",
    "    global_steps_list = []\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for text, label in train_loader:\n",
    "            optimizer.zero_grad()        \n",
    "            encoded_list = [tokenizer.encode(t, add_special_tokens=True) for t in text]\n",
    "            padded_list =  [e + [0] * (512-len(e)) for e in encoded_list]\n",
    "        \n",
    "            sample = torch.tensor(padded_list)\n",
    "            sample, label = sample.to(device), label.to(device)\n",
    "            labels = torch.tensor(label)\n",
    "            outputs = model(sample, labels=labels)\n",
    "            loss, logits = outputs\n",
    "\n",
    "            pred = torch.argmax(F.softmax(logits), dim=1)\n",
    "            correct = pred.eq(labels)\n",
    "            total_correct += correct.sum().item()\n",
    "            total_len += len(labels)\n",
    "            running_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()        \n",
    "            global_step += 1\n",
    "\n",
    "            if global_step % eval_every == 0:\n",
    "                model.eval()\n",
    "                with torch.no_grad():                    \n",
    "                    for text, label in valid_loader:\n",
    "                        encoded_list = [tokenizer.encode(t, add_special_tokens=True) for t in text]\n",
    "                        padded_list =  [e + [0] * (512-len(e)) for e in encoded_list]        \n",
    "                        sample = torch.tensor(padded_list)\n",
    "                        sample, label = sample.to(device), label.to(device)\n",
    "                        labels = torch.tensor(label)\n",
    "                        outputs = model(sample, labels=labels)\n",
    "                        loss, logits = outputs                        \n",
    "                        valid_running_loss += loss.item()\n",
    "\n",
    "                average_train_loss = running_loss / eval_every\n",
    "                average_valid_loss = valid_running_loss / len(valid_loader)\n",
    "                train_loss_list.append(average_train_loss)\n",
    "                valid_loss_list.append(average_valid_loss)\n",
    "                global_steps_list.append(global_step)\n",
    "\n",
    "                running_loss = 0.0                \n",
    "                valid_running_loss = 0.0\n",
    "                model.train()\n",
    "\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}'\n",
    "                      .format(epoch+1, num_epochs, global_step, num_epochs*len(train_loader),\n",
    "                              average_train_loss, average_valid_loss))\n",
    "                \n",
    "                if best_valid_loss > average_valid_loss:\n",
    "                    best_valid_loss = average_valid_loss\n",
    "                    save_checkpoint('C:/Users/sshyu/OneDrive/바탕 화면/Euron/chap10/data/model.pt', model, best_valid_loss)\n",
    "                    save_metrics('C:/Users/sshyu/OneDrive/바탕 화면/Euron/chap10/data/metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n",
    "    \n",
    "    save_metrics('C:/Users/sshyu/OneDrive/바탕 화면/Euron/chap10/data/metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n",
    "    print('훈련 종료!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e713bf1-9a95-4c84-badf-efbb47669c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 파라미터 조정 및 모델 훈련\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.05)\n",
    "train(model=model, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c79f0f8-5e0f-4563-a1d7-bc6dbb597397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 오차 정보 그래프로 확인\n",
    "train_loss_list, valid_loss_list, global_steps_list = load_metrics('C:/Users/sshyu/OneDrive/바탕 화면/Euron/chap10/data/metrics.pt')\n",
    "plt.plot(global_steps_list, train_loss_list, label='Train')\n",
    "plt.plot(global_steps_list, valid_loss_list, label='Valid')\n",
    "plt.xlabel('Global Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47f233a-2063-4049-895d-174a4aaaa6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 평가 함수 정의\n",
    "def evaluate(model, test_loader):\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for text, label in test_loader:\n",
    "            encoded_list = [tokenizer.encode(t, add_special_tokens=True) for t in text]\n",
    "            padded_list =  [e + [0] * (512-len(e)) for e in encoded_list]\n",
    "        \n",
    "            sample = torch.tensor(padded_list)\n",
    "            sample, label = sample.to(device), label.to(device)\n",
    "            labels = torch.tensor(label)\n",
    "            output = model(sample, labels=labels)\n",
    "            \n",
    "            _, output = output\n",
    "            y_pred.extend(torch.argmax(output, 1).tolist())\n",
    "            y_true.extend(labels.tolist())\n",
    "                    \n",
    "    print('Classification 결과:')\n",
    "    print(classification_report(y_true, y_pred, labels=[1,0], digits=4))\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[1,0])\n",
    "    ax= plt.subplot()\n",
    "    sns.heatmap(cm, annot=True, ax = ax, cmap='Blues', fmt=\"d\")\n",
    "    \n",
    "    ax.set_title('Confusion Matrix')\n",
    "    ax.set_xlabel('Predicted Labels')\n",
    "    ax.set_ylabel('True Labels')\n",
    "    ax.xaxis.set_ticklabels(['0', '1'])\n",
    "    ax.yaxis.set_ticklabels(['0', '1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0808cac1-1a02-4336-aada-1d095cbb6cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 평가\n",
    "best_model = model.to(device)\n",
    "load_checkpoint(''C:/Users/sshyu/OneDrive/바탕 화면/Euron/chap10/data/model.pt', best_model)\n",
    "evaluate(best_model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
