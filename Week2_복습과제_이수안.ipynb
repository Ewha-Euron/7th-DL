{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. A Single Neuron"
      ],
      "metadata": {
        "id": "Zr-LuLwE95z9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is Deep Learning?**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Deep learning is an approach to machine learning characterized by deep stacks of computations. Neural networks are composed of neurons, where each neuron individually performs only a simple computation. The power of a neural network comes instead from the complexity of the connections these neurons can form."
      ],
      "metadata": {
        "id": "_O5_9zkW92sq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The Linear Unit**\n",
        "\n",
        "---\n",
        "\n",
        "Input is `x`. Its connection to the neuron has a weight of `w`. When a value flows through a connection, you multiply the value by the connection's weight w. In this case, what reaches the neuron is w*x. The `b` is a special kind of weight bias. The bias doesn't have any input data associated with it instead, we put a 1 in the diagram so that the value that reaches the neuron is just 1* b. The `y` is the value the neuron ultimately outputs. This nueron's activation is `y = w * x + b` ."
      ],
      "metadata": {
        "id": "NhHqceRV_CtL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multiple Inputs**\n",
        "\n",
        "---\n",
        "\n",
        "We can add more input connections to the neuron. To find the output, we multiply each input to its connection weight and add them all together. The formula would look like\n",
        "\n",
        "```\n",
        "y = w0x0 + w1x1 + w2x2 + b\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "L6nmf2F-AJGC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Linear Units in Keras**"
      ],
      "metadata": {
        "id": "I9TnC5jXApWB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XUTiDR8f91uk"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(units=1, input_shape=[3])\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Neural Networks"
      ],
      "metadata": {
        "id": "mwHsvikfBFBf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Layers**\n",
        "\n",
        "---\n",
        "Neural networks typically organize their neurons into layers. When we collect together linear units having a common set of inputs we get a dense layer.\n"
      ],
      "metadata": {
        "id": "6R5PHoxHBKCw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The Activation Function**\n",
        "\n",
        "---\n",
        "However, two dense layers with nothing in between are no better than a single dense layer by itself. What we need is something *nonlinear*. What we need are activation functions. An activation function is some function we apply to each of a layer's outputs. The most common one is the rectifier function which has a graph that's a line with the negative part rectified to zero.\n"
      ],
      "metadata": {
        "id": "NN0CanO1BeTh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stacking Dense Layers**\n",
        "\n",
        "---\n",
        "\n",
        "After adding some nonlinearity, now we can stack layers to get complex data transformations. The layers before the output layer are sometimes called hidden."
      ],
      "metadata": {
        "id": "PTnfcUKmBepp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Building Sequential Models**\n",
        "\n",
        "---\n",
        "\n",
        "The `Sequential` model we've been using will connect together a list of layers in order from first to last. The first layer gets the input, the last layer produces the output.\n",
        "\n"
      ],
      "metadata": {
        "id": "a2kzNGgoN_e1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(units=4, activation='relu', input shape=[2]),\n",
        "    layers.Dense(units=3, activation='relu'),\n",
        "    layers.Dense(units=1),\n",
        "])"
      ],
      "metadata": {
        "id": "BL8L7dPwOXOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stochastic Gradient Descent"
      ],
      "metadata": {
        "id": "ws8ZDI4JN_5C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The Loss Function**\n",
        "\n",
        "---\n",
        "\n",
        "The loss function measures the disparity between the target's true value and the value the model predicts. A common loss function for regression problems is the mean absolute error or MAE. Besides MAE, other loss functions for regression problems are mean-squared error or the Huber loss."
      ],
      "metadata": {
        "id": "BnlPJL7mP9kc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The Optimizer - Stochastic Gradient Descent**\n",
        "\n",
        "---\n",
        "\n",
        "We need to inform the network how to solve the problem. This is the job of the optimizer. The optimizer adjusts the weights to minimize the loss. All of the optimization algorithms used in deep learning belong to a family called stochastic gradient descent.\n",
        "1. Sample some training data and run it through the network to make predictions\n",
        "2. Measure the loss between the predictions and the true values.\n",
        "3. Finally, adjust the weights in a direction that makes the loss smaler.\n",
        "4. Repeat this.\n",
        "Each iteration's sample of training data is called a minibatch while a complete round of the training data is called an epoch."
      ],
      "metadata": {
        "id": "0HYxDGdEP9g8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Learning Rate and Batch size**\n",
        "\n",
        "---\n",
        "\n",
        "The size of shifts is determinde by the learning rate. Smaller learning rate means the network needs to see more minibatches. The learning rate and the size of the minibatches affects SGD training process the most."
      ],
      "metadata": {
        "id": "6f-anaYfP9ee"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Adding the Loss and Optimizer**"
      ],
      "metadata": {
        "id": "-gxX_EWwP9cC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"mae\",\n",
        ")"
      ],
      "metadata": {
        "id": "n7QS-orEUUaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example - Red Wine Quality**"
      ],
      "metadata": {
        "id": "e31HALIEP9Zl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "red_wine = pd.read_csv('../input/dl-course-data/red-wine.csv')\n",
        "\n",
        "df_train = red_wine.sample(frac=0.7, random_state=0)\n",
        "df_valid = red_wine.drop(df_train.index)\n",
        "display(df_train.head(4))\n",
        "\n",
        "max_ = df_train.max(axis=0)\n",
        "min_ = df_train.min(axis=0)\n",
        "df_train = (df_train - min_) / (max_ - min_)\n",
        "df_valid = (df_valid - min_) / (max_ - min_)\n",
        "\n",
        "X_train = df_train.drop('quality', axis=1)\n",
        "X_valid = df_valid.drop('quality', axis=1)\n",
        "y_train = df_train['quality']\n",
        "y_valid = df_valid['quality']"
      ],
      "metadata": {
        "id": "fxjebeM8UlC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)"
      ],
      "metadata": {
        "id": "6gWMwDCQUxQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(512, activation='relu', input_shape=[11]),\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.Dense(1),\n",
        "])"
      ],
      "metadata": {
        "id": "4AY9wXxbUxMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='mae',\n",
        ")"
      ],
      "metadata": {
        "id": "-tzPXVd0UxHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_valid, y_valid),\n",
        "    batch_size=256,\n",
        "    epochs=10,\n",
        ")"
      ],
      "metadata": {
        "id": "c5E56mQNU3xA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# convert the training history to a dataframe\n",
        "history_df = pd.DataFrame(history.history)\n",
        "# use Pandas native plot method\n",
        "history_df['loss'].plot();"
      ],
      "metadata": {
        "id": "JCzs0pvDU5v9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overfitting and Underfitting"
      ],
      "metadata": {
        "id": "GHiEsE7DP9XJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpreting the Learning Curves**\n",
        "\n",
        "---\n",
        "\n",
        "The signal is the part that generalizes, the part that can help our model make predictions from new data. The noise is that part that is only true of the training data. We train a model by choosing weights or parameters that minimize the loss on a training set. To accurately assess a model's performance, we need validation data. Learning curves are the plots of the loss on the training set and validation set."
      ],
      "metadata": {
        "id": "fc5iwzIAVDDZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Underfitting** the training set is when the loss is not as low as it could be because the model hasn't learned enough *siganl*. **Overfitting** the training set is when the loss is not as low as it could be because the model learned too much *noise*."
      ],
      "metadata": {
        "id": "JhZbA82_W82t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Capacity**\n",
        "\n",
        "---\n",
        "\n",
        "A model's capacity refers to the size and complexity of the patterns it is able to learn. You can increase the capacity of a network either by making it wider or making it deeper."
      ],
      "metadata": {
        "id": "zySPfTmBX6gP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Sequential([\n",
        "    layers.Dense(16, activation='relu'),\n",
        "    layers.Dense(1),\n",
        "])\n",
        "\n",
        "wider = keras.Sequential([\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(1),\n",
        "])\n",
        "\n",
        "deeper = keras.Sequential([\n",
        "    layers.Dense(16, activation='relu'),\n",
        "    layers.Dense(16, activation='relu'),\n",
        "    layers.Dense(1),\n",
        "])"
      ],
      "metadata": {
        "id": "sW2F4fYyP9Dh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Early Stopping**\n",
        "\n",
        "---\n",
        "\n",
        "We can stop the training whenever it seems the validation loss isn't decreasing anymore. This is called early stopping. Once we detect that the validation loss is starting to rise again, we can reset the weights back to where the minimum occured. Training with early stopping also means we're in less danger of stopping the training too early."
      ],
      "metadata": {
        "id": "fYyyGpjdYNNG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Adding Early Stopping**"
      ],
      "metadata": {
        "id": "m9ZBdubuY6mP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stopping = EarlyStopping(\n",
        "    min_delta=0.001,\n",
        "    patience=20,\n",
        "    restore_best_weights=True,\n",
        ")"
      ],
      "metadata": {
        "id": "vUstkTr_Y-vS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example**"
      ],
      "metadata": {
        "id": "t1LdqIZfZELS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display\n",
        "\n",
        "red_wine = pd.read_csv('../input/dl-course-data/red-wine.csv')\n",
        "\n",
        "df_train = red_wine.sample(frac=0.7, random_state=0)\n",
        "df_valid = red_wine.drop(df_train.index)\n",
        "display(df_train.head(4))\n",
        "\n",
        "max_ = df_train.max(axis=0)\n",
        "min_ = df_train.min(axis=0)\n",
        "df_train = (df_train - min_) / (max_ - min_)\n",
        "df_valid = (df_valid - min_) / (max_ - min_)\n",
        "\n",
        "X_train = df_train.drop('quality', axis=1)\n",
        "X_valid = df_valid.drop('quality', axis=1)\n",
        "y_train = df_train['quality']\n",
        "y_valid = df_valid['quality']"
      ],
      "metadata": {
        "id": "jRNsSsw-ZG6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, callbacks\n",
        "\n",
        "early_stopping = callbacks.EarlyStopping(\n",
        "    min_delta=0.001,\n",
        "    patience=20,\n",
        "    restore_best_weights=True,\n",
        ")\n",
        "\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(512, activation='relu', input_shape=[11]),\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.Dense(1),\n",
        "])\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='mae',\n",
        ")"
      ],
      "metadata": {
        "id": "33Gmd-pSZNjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_valid, y_valid),\n",
        "    batch_size=256,\n",
        "    epochs=500,\n",
        "    callbacks=[early_stopping],\n",
        "    verbose=0,\n",
        ")\n",
        "\n",
        "history_df = pd.DataFrame(history.history)\n",
        "history_df.loc[:, ['loss', 'val_loss']].plot();\n",
        "print(\"Minimum validation loss: {}\".format(history_df['val_loss'].min()))"
      ],
      "metadata": {
        "id": "bwYbksNbZT1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dropout and Batch Normalization"
      ],
      "metadata": {
        "id": "TTpRVR_LZbG7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dropout**\n",
        "\n",
        "---\n",
        "\n",
        "Dropout layer can help correct overfitting. To break up these conspiracies, we randomly drop out some fraction of a layer's input units every step of training, making it much harder for the network to learn those spurious patterns in the training data. Instead, it has to search for broad, general patterns, whose weight patterns tend to be more robust."
      ],
      "metadata": {
        "id": "lP_-TJVhZbEN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's Add Dropout**"
      ],
      "metadata": {
        "id": "gH9Bi_RGZbBa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keras.Sequential([\n",
        "    # ...\n",
        "    layers.Dropout(rate=0.3),\n",
        "    layers.Dense(16),\n",
        "    # ...\n",
        "])\n"
      ],
      "metadata": {
        "id": "4pmzQey3ZiCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Batch Normalization**\n",
        "\n",
        "---\n",
        "\n",
        "Batch normalization can help correct training that is slow or unstable. A batch normalization layer looks at each batch as it comes in, first normalizing the batch with its own mean and standard deviation, and then also putting the data on a new scale with two trainable rescaling parameters. Batchnorm, in effect, performs a kind of coordinated rescaling of its inputs."
      ],
      "metadata": {
        "id": "4XWAkjV1Za-j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Adding Batch Normalization**"
      ],
      "metadata": {
        "id": "WWflSPoMZa7_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layers.Dense(16, activation='relu'),\n",
        "layers.BatchNormalization(),\n",
        "#Either way is fine\n",
        "layers.Dense(16),\n",
        "layers.BatchNormalization(),\n",
        "layers.Activation('relu'),"
      ],
      "metadata": {
        "id": "fooSlB5Raowt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example**"
      ],
      "metadata": {
        "id": "PL91StMaZa5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.style.use('seaborn-whitegrid')\n",
        "\n",
        "plt.rc('figure', autolayout=True)\n",
        "plt.rc('axes', labelweight='bold', labelsize='large',\n",
        "       titleweight='bold', titlesize=18, titlepad=10)\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "red_wine = pd.read_csv('../input/dl-course-data/red-wine.csv')\n",
        "\n",
        "df_train = red_wine.sample(frac=0.7, random_state=0)\n",
        "df_valid = red_wine.drop(df_train.index)\n",
        "\n",
        "X_train = df_train.drop('quality', axis=1)\n",
        "X_valid = df_valid.drop('quality', axis=1)\n",
        "y_train = df_train['quality']\n",
        "y_valid = df_valid['quality']"
      ],
      "metadata": {
        "id": "eTPS1Ib_awd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(1024, activation='relu', input_shape=[11]),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dense(1024, activation='relu'),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dense(1024, activation='relu'),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dense(1),\n",
        "])"
      ],
      "metadata": {
        "id": "nmPekYZTa5Gd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='mae',\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_valid, y_valid),\n",
        "    batch_size=256,\n",
        "    epochs=100,\n",
        "    verbose=0,\n",
        ")\n",
        "\n",
        "history_df = pd.DataFrame(history.history)\n",
        "history_df.loc[:, ['loss', 'val_loss']].plot();"
      ],
      "metadata": {
        "id": "wYVKgl8wa7GV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Binary Classification"
      ],
      "metadata": {
        "id": "XvDoxLfEZa2N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Accuracy and Cross-Entropy**\n",
        "\n",
        "---\n",
        "\n",
        "Accuracy is one of the many metrics in use for measuring success on a classification problem. Accuracy is the ratio of correct predictions to total predictions. The problem with accuracy is that it can't be used as a loss function. Cross-entropy function is the substitute for this. Cross-entropy is a sort of measure for the distance from one probability distribution to another."
      ],
      "metadata": {
        "id": "466z5tzSbEru"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Making Probabilities with the Sigmoid Function**\n",
        "\n",
        "---\n",
        "\n",
        "The cross-entropy and accuracy functions both require probabilities as inputs, meaning, numbers from 0 to 1. To convert them, we attach sigmoid activation. To get the final class prediction, we define a threshold probability. Typically this will be 0.5."
      ],
      "metadata": {
        "id": "JRw1rcI9bioy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example**"
      ],
      "metadata": {
        "id": "eaMi5sRFb4Xp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ion = pd.read_csv('../input/dl-course-data/ion.csv', index_col=0)\n",
        "display(ion.head())\n",
        "\n",
        "df = ion.copy()\n",
        "df['Class'] = df['Class'].map({'good': 0, 'bad': 1})\n",
        "\n",
        "df_train = df.sample(frac=0.7, random_state=0)\n",
        "df_valid = df.drop(df_train.index)\n",
        "\n",
        "max_ = df_train.max(axis=0)\n",
        "min_ = df_train.min(axis=0)\n",
        "\n",
        "df_train = (df_train - min_) / (max_ - min_)\n",
        "df_valid = (df_valid - min_) / (max_ - min_)\n",
        "df_train.dropna(axis=1, inplace=True) # drop the empty feature in column 2\n",
        "df_valid.dropna(axis=1, inplace=True)\n",
        "\n",
        "X_train = df_train.drop('Class', axis=1)\n",
        "X_valid = df_valid.drop('Class', axis=1)\n",
        "y_train = df_train['Class']\n",
        "y_valid = df_valid['Class']"
      ],
      "metadata": {
        "id": "P289jT7Zb3bi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(4, activation='relu', input_shape=[33]),\n",
        "    layers.Dense(4, activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid'),\n",
        "])"
      ],
      "metadata": {
        "id": "TVffF2F9b_n5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['binary_accuracy'],\n",
        ")"
      ],
      "metadata": {
        "id": "50zNUt4xcBKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = keras.callbacks.EarlyStopping(\n",
        "    patience=10,\n",
        "    min_delta=0.001,\n",
        "    restore_best_weights=True,\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_valid, y_valid),\n",
        "    batch_size=512,\n",
        "    epochs=1000,\n",
        "    callbacks=[early_stopping],\n",
        "    verbose=0,\n",
        ")"
      ],
      "metadata": {
        "id": "_tTMigYmcC2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_df = pd.DataFrame(history.history)\n",
        "history_df.loc[5:, ['loss', 'val_loss']].plot()\n",
        "history_df.loc[5:, ['binary_accuracy', 'val_binary_accuracy']].plot()\n",
        "\n",
        "print((\"Best Validation Loss: {:0.4f}\" +\\\n",
        "      \"\\nBest Validation Accuracy: {:0.4f}\")\\\n",
        "      .format(history_df['val_loss'].min(),\n",
        "              history_df['val_binary_accuracy'].max()))"
      ],
      "metadata": {
        "id": "WjeJaOsecFho"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}