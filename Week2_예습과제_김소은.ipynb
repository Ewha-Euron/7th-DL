{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d3c3705-bbc0-46e2-a831-7fe94996a5fb",
   "metadata": {},
   "source": [
    "#### 소프트맥스 함수 (softmax)\n",
    "- 입력 값을 0~1 사이에 출력되도록 정규화하여 출력 값들의 총합이 항상 1이 되도록 함\n",
    "- 보통 딥러닝에서 출력 노드의 활성화 함수로 많이 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abb7d1cd-be46-4da6-8f35-58795723468a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_hidden, n_output):\n",
    "    \tsuper(Net, self).__init__()\n",
    "    \tself.hidden = torch.nn.Linear(n_feature, n_hidden) #은닉층\n",
    "    \tself.relu = torch.nn.ReLu(inplace=True)\n",
    "    \tself.out = torch.nn.Linear(n_hidden, n_output) #출력층\n",
    "    \tself.softmax = torch.nn.Softmax(dim=n_output)\n",
    "    def forward(self, x):\n",
    "    \tx = self.hidden(x)\n",
    "    \tx = self.relu(x) #은닉층을 위한 렐루 활성화 함수\n",
    "    \tx = self.out(x)\n",
    "    \tx = self.softmax(x) #출력층을 위한 소프트맥스 활성화 함수\n",
    "    \treturn x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86d0bfc-b8d6-450b-a8ad-561e5cfcec3c",
   "metadata": {},
   "source": [
    "#### 평균 제곱 오차 (MSE) \n",
    "- 실제 값과 예측 값의 차이(error)를 제곱하여 평균을 낸 것\n",
    "- 작을 수록 → 예측력 good\n",
    "- 회귀에서 손실 함수로 주로 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2116742a-3de6-4d52-acec-f4fde3169afa",
   "metadata": {},
   "source": [
    "임시 모델 사용!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29ea767a-c72d-48ba-a5a9-5940b2294a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "model = nn.Linear(in_features=1, out_features=1, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "09ed0090-af6b-4d3d-9d79-e7263e5f374f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1.0], [2.0], [3.0]])\n",
    "y = torch.tensor([[2.0], [4.0], [6.0]])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a5f886ac-490f-4a06-bee3-092496e5a702",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "y_pred = model(x)\n",
    "loss = loss_fn(y_pred, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb4f093-ddcb-423c-9c2c-7b67b881e588",
   "metadata": {},
   "source": [
    "#### 크로스 엔트로피 오차 (CEE)\n",
    "- 분류 문제에서 원-핫 인코딩 했을 때만 사용할 수 있는 오차 계산법\n",
    "- 일반적으로 분류 문제에서 데이터의 출력을 0과 1로 구분하기 위해 시그모이드 함수 사용 → 함수에 포함된 자연 상수 e 때문에 평균 제곱 오차 적용하면 울퉁불퉁한 그래프 출력 —> 크로스 엔트로피 손실 함수로 해결!\n",
    "- CEE 적용 경우 경사 하강법 과정에서 학습이 지역 최소점에서 멈출 수 있음 → 방지하고자 자연 상수 e에 반대되는 자연 로그를 모델의 출력 값에 취함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7080daea-8379-48fd-a4d6-fdf7dedaf9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.randn(5, 6, requires_grad=True) #torch.randn은 평균이 0이고 표준편차가 1인 가우시안 정규분포를 이용하여 숫자 생성\n",
    "target = torch.empty(5, dtype=torch.long).random_(6) #torch.empty는 dtype torch.float32의 랜덤한 값으로 채워진 텐서를 반환\n",
    "output = loss(input, target)\n",
    "output.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23039ac9-9e54-498e-9216-e1386a1776ed",
   "metadata": {},
   "source": [
    "#### 드롭아웃(dropout)\n",
    "- 학습 과정 중 임의로 일부 노드들을 학습에서 제외\n",
    "- 과적합 해결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cdb67c67-ff5f-4024-92fe-f1c44ba404c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropoutModel(torch.nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(DropoutModel, self).__init__()\n",
    "\t\tself.layer1 = torch.nn.Linear(784, 1200)\n",
    "\t\tself.dropout1 = torch.nn.Dropout(0.5) #50%의 노드를 무작위로 선택해 사용하지 않겠다는 의미\n",
    "\t\tself.layer2 = torch.nn.Linear(1200, 1200)\n",
    "\t\tself.dropout2 = torch.nn.Dropout(0.5)\n",
    "\t\tself.layer3 = torch.nn.Linear(1200, 10)\n",
    "\t\t\n",
    "\tdef forward(self, x):\n",
    "\t\tx = F.relu(self.layer1(x))\n",
    "\t\tx = self.dropout1(x)\n",
    "\t\tx = F.relu(self.layer2(x))\n",
    "\t\tx = self.dropout2(x)\n",
    "\t\treturn self.layer3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd0c08f-b0da-4a78-960b-e754695ab1e1",
   "metadata": {},
   "source": [
    "#### 미니 배치 경사 하강법 (mini-batch gradient descent) \n",
    "- 전체 데이터셋을 미니 배치 여러개로 나누고 → 미니 배치 한 개마다 기울기 구하고 → 그것의 평균 기울기 이용해 모델 업데이트해서 학습\n",
    "- 전체 데이터 계산하는 것보다 빠름, 확률적 경사 하강법보다 안정적"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c138cf40-77e6-4fda-a0ab-531fce641bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "class CustomDataset(Dataset):\n",
    "\tdef __init__(self):\n",
    "\t\tself.x_data = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n",
    "\t\tself.y_data = [[12], [18], [11]]\n",
    "\tdef __len__(self):\n",
    "\t\t\treturn len(self.x_data)\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\t\tx = torch.FloatTensor(self.x_data[idx])\n",
    "\t\t\ty = torch.FloatTensor(self.y_data[idx])\n",
    "\t\t\treturn x, y\n",
    "dataset = CustomDataset()\n",
    "dataloader = DataLoader(\n",
    "\tdataset, #데이터셋\n",
    "\tbatch_size=2, #미니 배치 크기로 2의 제곱수를 사용\n",
    "\tshuffle=True, #데이터 불러올 때마다 랜덤으로 섞기\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9606c0b6-b671-4711-b5d6-194761426b62",
   "metadata": {},
   "source": [
    "#### 아다그라드(Adagrad, Adaptive gradient)\n",
    "- 변수(가중치)의 업데이트 횟수에 따라 학습률 조정하는 방법\n",
    "- 많이 변화하지 않는 변수들의 학습률은 크게, 많이 변화하는 변수들의 학습률은 작게 함 → 많이 변화한 변수는 최적 값에 근접했을 것이라는 가정합에 작은 크기로 이동, 세밀하게 값 조정 → 적게 변화한 변수들 학습률 크게 하여 빠르게 오차 값 줄이고자 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0831f789-232e-43e2-a217-b9eb956c676a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adagrad(model.parameters(), lr=0.1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cef9738-ead9-419e-810e-44e69f9225f2",
   "metadata": {},
   "source": [
    "#### 아다델타(Adadelta, Adaptive delta)\n",
    "- 아다그라드에서 G 값이 커짐에 따라 학습을 멈추는 문제 해결 위한 방법\n",
    "- 아다그라드의 수식에서 학습률을 D 함수(가중치의 변화량 크기를 누적한 값)로 변환했기 때문에 학습률에 대한 하이퍼파라미터 필요 x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "75dec56f-b6b7-4c81-bdd0-685e814e427d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adadelta(model.parameters(), lr=1.0) #학습률 기본값은 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80607600-808b-4985-8d09-b0efadf0ac4e",
   "metadata": {},
   "source": [
    "#### 알엠에스프롭(RMSProp)\n",
    "- 아다그라드의 G(i) 값이 무한히 커지는 것을 방지하고자 제안된 방법 → G 함수에서 y(감마)만 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "32806ebe-1e81-40ed-bf83-4c28fa7ff98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=0.01) #학습률 기본값 1e-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007d651f-fd0f-46be-99dc-b250b706aa47",
   "metadata": {},
   "source": [
    "#### 모멘텀(Momentum)\n",
    "- 경사 하강법과 마찬가지로 매번 기울기 구하지만, 가중치를 수정하기 전에 이전 수정 방향(+,-)을 참고하여 같은 방향으로 일정한 비율 수정하는 방법\n",
    "- 수정 방향 (+,-) —> 지그재그 현장 less + 일정 비율만큼 다음 값 결정하므로 관성 효과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5d096de0-0127-48e0-ae49-bd1be36d8f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a6d74a-1fd5-4c67-8b93-77650a0a6d10",
   "metadata": {},
   "source": [
    "#### 네스테로프 모멘텀(Nesterov Accelerated Gradient, NAG)\n",
    "- 모멘텀 값이 적용된 지점에서 기울기 값을 계산\n",
    "    - *모멘텀 값과 기울기 값이 더해져 실제 값을 만드는 기존 모멘텀과 달르다!*\n",
    "- 모멘텀으로 절반 정도 이동 후 어떤 방식으로 이동해야 하는지 다시 계산하여 결정 → 모멘텀의 단점 극복 (멈추어야 할 시점에서도 관성에 의해 훨씬 멀리 갈 수 있다)\n",
    "- 모멘텀 방법의 이점인 **빠른 이동 속도** 그대로 가져가면서 **멈추어야 할 적절한 시점**에서 **제공** 거는 데 훨씬 용이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dd26310a-84a4-4609-8811-51413f2c2782",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9,\n",
    "\t\t\t\t\t\tnesterov=True) #nesterov 기본값은 False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb59eb2b-1ae5-47f8-9cc4-a3d65f7b418f",
   "metadata": {},
   "source": [
    "#### 아담(Adam, Adaptive Moment Estimation) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
