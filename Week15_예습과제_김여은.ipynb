{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "Gy1vEHpe9kuY",
      "metadata": {
        "id": "Gy1vEHpe9kuY"
      },
      "source": [
        "# 10.1 임베딩"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3gjvn01supFt",
      "metadata": {
        "id": "3gjvn01supFt"
      },
      "source": [
        "## 10.1.1 희소 표현 기반 임베딩"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d93e9e0d",
      "metadata": {
        "id": "d93e9e0d"
      },
      "outputs": [],
      "source": [
        "# 원핫 인코딩 적용\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "class2=pd.read_csv(\"class2.csv\") # 데이터셋을 메모리로 로딩\n",
        "\n",
        "from sklearn import preprocessing\n",
        "label_encoder = preprocessing.LabelEncoder() # 데이터를 인코딩하는 데 사용. 다음의 OneHotEncoder와 함께 사용\n",
        "onehot_encoder = preprocessing.OneHotEncoder() # 데이터를 숫자 형식으로 표현\n",
        "\n",
        "train_x = label_encoder.fit_transform(class2['class2'])\n",
        "train_x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "veItHq30-X8M",
      "metadata": {
        "id": "veItHq30-X8M"
      },
      "source": [
        "## 10.1.2 횟수기반 임베딩"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5298928",
      "metadata": {
        "id": "d5298928"
      },
      "outputs": [],
      "source": [
        "# 코퍼스에 카운터 벡터 적용\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "corpus = [\n",
        "    'This is last chance.',\n",
        "    'and if you do not have this chance.',\n",
        "    'you will never get any chance.',\n",
        "    'will you do get this one?',\n",
        "    'please, get this chance',\n",
        "]\n",
        "vect = CountVectorizer()\n",
        "vect.fit(corpus)\n",
        "vect.vocabulary_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cda3fe3e",
      "metadata": {
        "id": "cda3fe3e"
      },
      "outputs": [],
      "source": [
        "# 배열 변환\n",
        "vect.transform(['you will never get any chance.']).toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b305aef",
      "metadata": {
        "id": "6b305aef"
      },
      "outputs": [],
      "source": [
        "# 불용어를 제거한 카운터 벡터\n",
        "vect = CountVectorizer(stop_words=[\"and\", \"is\", \"please\", \"this\"]).fit(corpus) # stop_words를 사용하여 is, not, an 같은 불용어 제거\n",
        "vect.vocabulary_"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oKA6yK_iu3bT",
      "metadata": {
        "id": "oKA6yK_iu3bT"
      },
      "source": [
        "### TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fea52e5",
      "metadata": {
        "id": "8fea52e5"
      },
      "outputs": [],
      "source": [
        "# TF-IDF를 적용한 후 행렬로 표현\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "doc = ['I like machine learning', 'I love deep learning', 'I run everyday']\n",
        "tfidf_vectorizer = TfidfVectorizer(min_df=1)\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(doc)\n",
        "doc_distance = (tfidf_matrix * tfidf_matrix.T)\n",
        "print ('유사도를 위한', str(doc_distance.get_shape()[0]), 'x', str(doc_distance.get_shape()[1]), 'matrix를 만들었습니다.')\n",
        "print(doc_distance.toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Dp8BPEZB-rhR",
      "metadata": {
        "id": "Dp8BPEZB-rhR"
      },
      "source": [
        "## 10.1.3 예측기반 임베딩"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wmcOz0VU_ROY",
      "metadata": {
        "id": "wmcOz0VU_ROY"
      },
      "outputs": [],
      "source": [
        "# nltk 설치\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RL2E_RP3_SB0",
      "metadata": {
        "id": "RL2E_RP3_SB0"
      },
      "outputs": [],
      "source": [
        "# nltk 패키지 불러오기\n",
        "import nltk\n",
        "nltk.download(\"all-nltk\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Yy5LFujavCKn",
      "metadata": {
        "id": "Yy5LFujavCKn"
      },
      "source": [
        "### 워드투벡터"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d68eed27",
      "metadata": {
        "id": "d68eed27"
      },
      "outputs": [],
      "source": [
        "# 데이터셋을 메모리로 로딩하고 토큰화 적용\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import warnings\n",
        "warnings.filterwarnings(action = 'ignore')\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "sample = open(\"peter.txt\", \"r\", encoding='UTF8') # 피터팬 데이터셋 로딩\n",
        "s = sample.read()\n",
        "\n",
        "f = s.replace(\"\\n\", \" \") # 줄바꿈을 공백으로 변환\n",
        "data = []\n",
        "\n",
        "for i in sent_tokenize(f): # 로딩한 파일의 각 문장마다 반복\n",
        "    temp = []\n",
        "    for j in word_tokenize(i): # 문장을 단어로 토큰화\n",
        "        temp.append(j.lower()) # 토큰화된 단어를 소문자로 변환하여 temp에 저장\n",
        "    data.append(temp)\n",
        "\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tPM504e0vFqg",
      "metadata": {
        "id": "tPM504e0vFqg"
      },
      "source": [
        "### CBOW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcbd3aed",
      "metadata": {
        "id": "bcbd3aed"
      },
      "outputs": [],
      "source": [
        "# 데이터셋에 CBOW 적용 후 peter와 wendy의 유사성 확인\n",
        "model1 = gensim.models.Word2Vec(data, min_count = 1,\n",
        "                                vector_size = 100, window = 5, sg=0)\n",
        "print(\"Cosine similarity between 'peter' \" +\n",
        "                 \"wendy' - CBOW : \",\n",
        "      model1.wv.similarity('wendy', 'wendy'))\n",
        "\n",
        "model1 = gensim.models.Word2Vec(data, min_count = 1,\n",
        "                                vector_size = 100, window = 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f6eac9a",
      "metadata": {
        "id": "9f6eac9a"
      },
      "outputs": [],
      "source": [
        "# peter와 hook 유사성 확인\n",
        "print(\"Cosine similarity between 'peter' \" +\n",
        "                 \"hook' - CBOW : \",\n",
        "      model1.wv.similarity('peter', 'hook'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0k43dy0EvJ5u",
      "metadata": {
        "id": "0k43dy0EvJ5u"
      },
      "source": [
        "### skip-gram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fb8edd9",
      "metadata": {
        "id": "4fb8edd9"
      },
      "outputs": [],
      "source": [
        "# 데이터셋에 skip gram 적용 후 peter와 wendy의 유사성 확인\n",
        "model2 = gensim.models.Word2Vec(data, min_count = 1, vector_size = 100,\n",
        "                                window = 5, sg = 1) # skip gram 모델 사용\n",
        "print(\"Cosine similarity between 'peter' \" +\n",
        "          \"wendy' - Skip Gram : \",\n",
        "    model2.wv.similarity('peter', 'wendy')) # 결과 출력"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8128bcb6",
      "metadata": {
        "id": "8128bcb6"
      },
      "outputs": [],
      "source": [
        "# peter와 hook의 유사성\n",
        "print(\"Cosine similarity between 'peter' \" +\n",
        "            \"hook' - Skip Gram : \",\n",
        "      model2.wv.similarity('peter', 'hook'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-SK2OGx2vNmG",
      "metadata": {
        "id": "-SK2OGx2vNmG"
      },
      "source": [
        "### 패스트텍스트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Pkp_gTEv66k-",
      "metadata": {
        "id": "Pkp_gTEv66k-"
      },
      "outputs": [],
      "source": [
        "# 라이브러리 및 데이터 호출\n",
        "from gensim.test.utils import common_texts\n",
        "from gensim.models import FastText\n",
        "\n",
        "model=FastText('..\\chap10\\data\\peter.txt', vector_size=4, window=3, min_count=1, epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ASHJR_jF66jg",
      "metadata": {
        "id": "ASHJR_jF66jg"
      },
      "outputs": [],
      "source": [
        "# peter, wendy에 대한 코사인 유사도\n",
        "sim_score=model.wv.similarity('peter', 'wendy')\n",
        "print(sim_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ScsqkkTx7MYs",
      "metadata": {
        "id": "ScsqkkTx7MYs"
      },
      "outputs": [],
      "source": [
        "# peter, hook에 대한 코사인 유사도\n",
        "sim_score=model.wv.similarity('peter', 'hook')\n",
        "print(sim_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FLQxjLDx7c2a",
      "metadata": {
        "id": "FLQxjLDx7c2a"
      },
      "outputs": [],
      "source": [
        "# 라이브러리와 사전 훈련된 모델 호출\n",
        "from __future__ import print_function\n",
        "from gensim.models import KeyedVectors # gensim은 자연어를 벡터로 변환하는 데 필요한 편의 기능을 제공하는 라이브러리입니다\n",
        "\n",
        "model_kr=KeyedVectors.load_word2vec_format('..\\chap10\\data\\wiki.ko.vec') # wiki.ko.vec 파일을 메모리로 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cTgz3Zm47wYP",
      "metadata": {
        "id": "cTgz3Zm47wYP"
      },
      "outputs": [],
      "source": [
        "# 노력과 유사한 단어와 유사도 확인\n",
        "find_similar_to='노력'\n",
        "\n",
        "for similar_word in model_kr.similar_by_word(find_similar_to):\n",
        "    print(\"Word: {0}, Similarity: {1:.2f}\".format(\n",
        "        similar_word[0], similar_word[1]\n",
        "    ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mVzjTtwl749C",
      "metadata": {
        "id": "mVzjTtwl749C"
      },
      "outputs": [],
      "source": [
        "# 동물, 육식동물에는 긍정적이지만 사람에는 부정적인 단어와 유사도 확인\n",
        "similarities=model_kr.most_similar(positive=['동물', '육식동물'], negative=['사람'])\n",
        "print(similarities)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xibBM4mjvTRK",
      "metadata": {
        "id": "xibBM4mjvTRK"
      },
      "source": [
        "## 10.1.4 횟수/예측 기반 임베딩"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5LQpigvYvVp9",
      "metadata": {
        "id": "5LQpigvYvVp9"
      },
      "source": [
        "### 글로브"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Sj7RCfDoNJiU",
      "metadata": {
        "id": "Sj7RCfDoNJiU"
      },
      "outputs": [],
      "source": [
        "# 라이브러리 및 데이터셋 로딩\n",
        "import numpy as np\n",
        "%matplotlib notebook\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "from sklearn.decomposition import PCA\n",
        "from gensim.test.utils import datapath, get_tmpfile\n",
        "from gensim.models import KeyedVectors\n",
        "import gensim.scripts.glove2word2vec import glove2word2vec\n",
        "\n",
        "glove_file=datapath('..\\chap10\\data\\glove.6B.100d.txt')\n",
        "word2vec_glove_file=get_tmpfile(\"glove.6B.100d.word2vec.txt\") # 글로브 데이터를 위한 워드투벡터 형태로 변환\n",
        "glove2word2vec(glove_file, word2vec_glove_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lV__Oeo6JskG",
      "metadata": {
        "id": "lV__Oeo6JskG"
      },
      "outputs": [],
      "source": [
        "# bill과 유사한 단어의 리스트를 반환\n",
        "model = KeyedVectors.load_word2vec_format(word2vec_glove_file) # load_word2vec_format 메서드를 이용해 word2vec.c 형식으로 벡터를 가져옴\n",
        "model.most_similar('bill') # 단어(bill) 기준으로 가장 유사한 단어들의 리스트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4543f02c",
      "metadata": {
        "id": "4543f02c"
      },
      "outputs": [],
      "source": [
        "# cherry와 유사한 단어의 리스트를 반환\n",
        "model.most_similar('cherry') # 단어(cherry) 기준으로 가장 유사한 단어들의 리스트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5fb5c1b",
      "metadata": {
        "id": "b5fb5c1b"
      },
      "outputs": [],
      "source": [
        "# cherry와 관련성이 없는 단어의 리스트를 반환\n",
        "model.most_similar(negative=['cherry']) # 단어(cherry)와 관련성이 없는 단어들을 반환"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "346acbc3",
      "metadata": {
        "id": "346acbc3"
      },
      "outputs": [],
      "source": [
        "# woman, king과 유사성이 높으면서 man과 관련성이 없는 단어 반환\n",
        "result=model.most_similar(positive=['woman', 'king'], negative=['man'])\n",
        "print(\"{}: {:.4f}\".format(*result[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72881522",
      "metadata": {
        "id": "72881522"
      },
      "outputs": [],
      "source": [
        "# australia, beer, france와 관련성이 있는 단어 반환\n",
        "def analogy(x1, x2, y1):\n",
        "    result = model.most_similar(positive=[y1, x2], negative=[x1])\n",
        "    return result[0][0]\n",
        "analogy('australia', 'beer', 'france')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a8a76cf",
      "metadata": {
        "id": "9a8a76cf"
      },
      "outputs": [],
      "source": [
        "# tall, tallest, long 단어 기반으로 새로운 단어 유추\n",
        "analogy('tall', 'tallest', 'long')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "820bf3a4",
      "metadata": {
        "id": "820bf3a4"
      },
      "outputs": [],
      "source": [
        "# breakfast cereal dinner lunch 중 유사도가 낮은 단어 반환\n",
        "print(model.doesnt_match(\"breakfast cereal dinner lunch\".split())) # 유사도가 가장 낮은 단어 반환"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QfSXQFZhG2nT",
      "metadata": {
        "id": "QfSXQFZhG2nT"
      },
      "source": [
        "# 10.2 트랜스포머 어텐션"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hjMem11avdR9",
      "metadata": {
        "id": "hjMem11avdR9"
      },
      "source": [
        "## 10.2.1 seq2seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ae5b052",
      "metadata": {
        "id": "6ae5b052"
      },
      "outputs": [],
      "source": [
        "# 라이브러리 호출\n",
        "from __future__ import unicode_literals, print_function, division\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import os\n",
        "import re\n",
        "import random\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4841b42",
      "metadata": {
        "id": "c4841b42"
      },
      "outputs": [],
      "source": [
        "# 데이터 준비\n",
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "MAX_LENGTH = 20\n",
        "\n",
        "class Lang: # 딕셔너리를 만들기 위한 클래스\n",
        "    def __init__(self): # 단어의 인덱스를 저장하기 위한 컨테이너를 초기화\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"} # SOS: 문장의 시작, EOS: 문장의 끝\n",
        "        self.n_words = 2\n",
        "\n",
        "    def addSentence(self, sentence): # 문장을 단어 단위로 분리한 후 컨테이너에 추가\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word): # 컨테이너에 단어가 없다면 추가되고, 있다면 카운트를 업데이트\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58ebe3bd",
      "metadata": {
        "id": "58ebe3bd"
      },
      "outputs": [],
      "source": [
        "# 데이터 정규화\n",
        "def normalizeString(df, lang):\n",
        "    sentence = df[lang].str.lower()\n",
        "    sentence = sentence.str.replace('[^A-Za-z\\s]+', '') # a-z, A-Z, ..., ?, ! 등을 제외하고 모두 공백으로 바꿈\n",
        "    sentence = sentence.str.normalize('NFD') # 유니코드 정규화 방식\n",
        "    sentence = sentence.str.encode('ascii', errors='ignore').str.decode('utf-8') # Unicode를 ASCII로 전환\n",
        "    return sentence\n",
        "\n",
        "def read_sentence(df, lang1, lang2):\n",
        "    sentence1 = normalizeString(df, lang1) # 데이터셋의 첫 번째 열(영어)\n",
        "    sentence2 = normalizeString(df, lang2) # 데이터셋의 두 번째 열(프랑스어)\n",
        "    return sentence1, sentence2\n",
        "\n",
        "def read_file(loc, lang1, lang2):\n",
        "    df = pd.read_csv(loc, delimiter='\\t', header=None, names=[lang1, lang2])\n",
        "    return df\n",
        "\n",
        "def process_data(lang1,lang2):\n",
        "    df = read_file('../chap10/data/%s-%s.txt' % (lang1, lang2), lang1, lang2) # 데이터셋 불러오기\n",
        "    sentence1, sentence2 = read_sentence(df, lang1, lang2)\n",
        "\n",
        "    input_lang = Lang()\n",
        "    output_lang = Lang()\n",
        "    pairs = []\n",
        "    for i in range(len(df)):\n",
        "        if len(sentence1[i].split(' ')) < MAX_LENGTH and len(sentence2[i].split(' ')) < MAX_LENGTH:\n",
        "            full = [sentence1[i], sentence2[i]] # 첫 번째와 두 번째 열을 합쳐서 저장\n",
        "            input_lang.addSentence(sentence1[i]) # 입력으로 영어 사용\n",
        "            output_lang.addSentence(sentence2[i]) # 출력으로 프랑스어 사용\n",
        "            pairs.append(full) # pairs에는 입력과 출력이 합쳐진 것을 사용\n",
        "\n",
        "    return input_lang, output_lang, pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0c3e7a2",
      "metadata": {
        "id": "f0c3e7a2"
      },
      "outputs": [],
      "source": [
        "# 텐서로 변환\n",
        "def indexesFromSentence(lang, sentence): # 문장을 단어로 분리하고 인덱스를 반환\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "def tensorFromSentence(lang, sentence): # 딕셔너리에서 단어에 대한 인덱스를 가져오고 문장 끝에 토큰을 추가\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "def tensorsFromPair(input_lang, output_lang, pair): # 입력과 출력 문장을 텐서로 변환하여 반환\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5cda608",
      "metadata": {
        "id": "e5cda608"
      },
      "outputs": [],
      "source": [
        "# 인코더 네트워크\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, embbed_dim, num_layers):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.input_dim = input_dim # 인코더에서 사용할 입력층\n",
        "        self.embbed_dim = embbed_dim # 인코더에서 사용할 임베딩 계층\n",
        "        self.hidden_dim = hidden_dim # 인코더에서 사용할 은닉층(이전 은닉층)\n",
        "        self.num_layers = num_layers # 인코더에서 사용할 GRU의 계층 개수\n",
        "        self.embedding = nn.Embedding(input_dim, self.embbed_dim) # 임베딩 계층 초기화\n",
        "        self.gru = nn.GRU(self.embbed_dim, self.hidden_dim, num_layers=self.num_layers) # 임베딩 차원, 은닉층 차원, GRU의 계층 개수를 이용하여 GRU 계층을 초기화\n",
        "\n",
        "    def forward(self, src):\n",
        "        embedded = self.embedding(src).view(1,1,-1) # 임베딩 처리\n",
        "        outputs, hidden = self.gru(embedded) # 임베딩 결과를 GRU 모델에 적용\n",
        "        return outputs, hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c8c263c",
      "metadata": {
        "id": "8c8c263c"
      },
      "outputs": [],
      "source": [
        "# 디코더 네트워크\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, hidden_dim, embbed_dim, num_layers):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.embbed_dim = embbed_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(output_dim, self.embbed_dim) # 임베딩 계층 초기화\n",
        "        self.gru = nn.GRU(self.embbed_dim, self.hidden_dim, num_layers=self.num_layers) # GRU 계층 초기화\n",
        "        self.out = nn.Linear(self.hidden_dim, output_dim) # 선형 계층 초기화\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        input = input.view(1, -1) # 입력을 (1,배치 크기)로 변경\n",
        "        embedded = F.relu(self.embedding(input))\n",
        "        output, hidden = self.gru(embedded, hidden)\n",
        "        prediction = self.softmax(self.out(output[0]))\n",
        "        return prediction, hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e32442e",
      "metadata": {
        "id": "2e32442e"
      },
      "outputs": [],
      "source": [
        "# seq2seq 네트워크\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device, MAX_LENGTH=MAX_LENGTH):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = encoder # 인코더 초기화\n",
        "        self.decoder = decoder # 디코더 초기화\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, input_lang, output_lang, teacher_forcing_ratio=0.5):\n",
        "\n",
        "        input_length = input_lang.size(0) # 입력 문자 길이(문장 단어 수)\n",
        "        batch_size = output_lang.shape[1]\n",
        "        target_length = output_lang.shape[0]\n",
        "        vocab_size = self.decoder.output_dim\n",
        "        outputs = torch.zeros(target_length, batch_size, vocab_size).to(self.device) # 예측된 출력을 저장하기 위한 변수 초기화\n",
        "\n",
        "        for i in range(input_length):\n",
        "            encoder_output, encoder_hidden = self.encoder(input_lang[i]) # 문장의 모든 단어를 인코딩\n",
        "        decoder_hidden = encoder_hidden.to(device) # 인코더의 은닉층을 디코더의 은닉층으로 사용\n",
        "        decoder_input = torch.tensor([SOS_token], device=device) # 첫번째 예측 단어 앞에 토큰(SOS) 추가\n",
        "\n",
        "        for t in range(target_length): # 현재 단어에서 출력 단어를 예측\n",
        "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
        "            outputs[t] = decoder_output\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            input = (output_lang[t] if teacher_force else topi) # teacher_force를 활성화하면 목표 단어를 다음 입력으로 사용\n",
        "            if(teacher_force == False and input.item() == EOS_token): # teacher_force를 활성화하지 않으면 자체 예측 값을 다음 입력으로 사용\n",
        "                break\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93d4ebc3",
      "metadata": {
        "id": "93d4ebc3"
      },
      "outputs": [],
      "source": [
        "# 모델의 오차 계산 함수 정의\n",
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "def Model(model, input_tensor, target_tensor, model_optimizer, criterion):\n",
        "    model_optimizer.zero_grad()\n",
        "    input_length = input_tensor.size(0)\n",
        "    loss = 0\n",
        "    epoch_loss = 0\n",
        "    output = model(input_tensor, target_tensor)\n",
        "    num_iter = output.size(0)\n",
        "\n",
        "    for ot in range(num_iter):\n",
        "        loss += criterion(output[ot], target_tensor[ot]) # 모델의 예측 결과와 정답(예상 결과)을 이용하여 오차 계산\n",
        "\n",
        "    loss.backward()\n",
        "    model_optimizer.step()\n",
        "    epoch_loss = loss.item() / num_iter\n",
        "    return epoch_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9768b4e8",
      "metadata": {
        "id": "9768b4e8"
      },
      "outputs": [],
      "source": [
        "# 모델 훈련 함수 정의\n",
        "def trainModel(model, input_lang, output_lang, pairs, num_iteration=20000):\n",
        "    model.train()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.01) # 옵티마이저로 SGD를 사용\n",
        "    criterion = nn.NLLLoss()\n",
        "    total_loss_iterations = 0\n",
        "\n",
        "    training_pairs = [tensorsFromPair(input_lang, output_lang, random.choice(pairs))\n",
        "                      for i in range(num_iteration)]\n",
        "\n",
        "    for iter in range(1, num_iteration+1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "        loss = Model(model, input_tensor, target_tensor, optimizer, criterion) # Model 객체를 이용하여 오차 계산\n",
        "        total_loss_iterations += loss\n",
        "\n",
        "        if iter % 5000 == 0: # 5000번째마다 오차 값에 대해 출력\n",
        "            avarage_loss= total_loss_iterations / 5000\n",
        "            total_loss_iterations = 0\n",
        "            print('%d %.4f' % (iter, avarage_loss))\n",
        "\n",
        "    torch.save(model.state_dict(), '../chap10/data/mytraining.pt')\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bd2079f",
      "metadata": {
        "id": "9bd2079f"
      },
      "outputs": [],
      "source": [
        "# 모델 평가\n",
        "def evaluate(model, input_lang, output_lang, sentences, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentences[0]) # 입력 문자열을 텐서로 변환\n",
        "        output_tensor = tensorFromSentence(output_lang, sentences[1]) # 출력 문자열을 텐서로 변환\n",
        "        decoded_words = []\n",
        "        output = model(input_tensor, output_tensor)\n",
        "\n",
        "        for ot in range(output.size(0)):\n",
        "            topv, topi = output[ot].topk(1) # 각 출력에서 가장 높은 값을 찾아 인덱스를 반환\n",
        "\n",
        "            if topi[0].item() == EOS_token:\n",
        "                decoded_words.append('<EOS>') # EOS 토큰을 만나면 평가를 멈춤\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi[0].item()]) # 예측 결과를 출력 문자열에 추가\n",
        "    return decoded_words\n",
        "\n",
        "def evaluateRandomly(model, input_lang, output_lang, pairs, n=10): # 훈련 데이터셋으로부터 임의의 문장을 가져와서 모델 평가\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs) # 임의로 문장을 가져옴\n",
        "        print('input {}'.format(pair[0]))\n",
        "        print('output {}'.format(pair[1]))\n",
        "        output_words = evaluate(model, input_lang, output_lang, pair) # 모델 평가 결과는 output_words에 저장\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('predicted {}'.format(output_sentence))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6814115",
      "metadata": {
        "id": "b6814115"
      },
      "outputs": [],
      "source": [
        "# 모델 훈련\n",
        "lang1 = 'eng' # 입력으로 사용할 영어\n",
        "lang2 = 'fra' # 출력으로 사용할 프랑스어\n",
        "input_lang, output_lang, pairs = process_data(lang1, lang2)\n",
        "\n",
        "randomize = random.choice(pairs)\n",
        "print('random sentence {}'.format(randomize))\n",
        "\n",
        "input_size = input_lang.n_words\n",
        "output_size = output_lang.n_words\n",
        "print('Input : {} Output : {}'.format(input_size, output_size)) # 입력과 출력에 대한 단어 수 출력\n",
        "\n",
        "embed_size = 256\n",
        "hidden_size = 512\n",
        "num_layers = 1\n",
        "num_iteration = 75000 # 75000번 반복하여 모델 훈련\n",
        "\n",
        "encoder = Encoder(input_size, hidden_size, embed_size, num_layers) # 인코더에 훈련 데이터셋을 입력하고 모든 출력과 은닉 상태를 저장\n",
        "decoder = Decoder(output_size, hidden_size, embed_size, num_layers) # 디코더의 첫번째 입력으로 SOS 토큰이 제공, 인코더의 마지막 은닉 상태가 디코더의 첫번째 은닉 상태로 제공\n",
        "\n",
        "model = Seq2Seq(encoder, decoder, device).to(device) # 인코더 디코더 모델(seq2seq) 객체 생성\n",
        "\n",
        "print(encoder)\n",
        "print(decoder)\n",
        "\n",
        "model = trainModel(model, input_lang, output_lang, pairs, num_iteration) # 모델 학습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa2807a7",
      "metadata": {
        "id": "fa2807a7"
      },
      "outputs": [],
      "source": [
        "# 임의의 문장에 대한 평가 결과\n",
        "evaluateRandomly(model, input_lang, output_lang, pairs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6032888a",
      "metadata": {
        "id": "6032888a"
      },
      "outputs": [],
      "source": [
        "# 어텐션이 적용된 디코더\n",
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size) # 임베딩 계층 초기화\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bb7ef62",
      "metadata": {
        "id": "9bb7ef62"
      },
      "outputs": [],
      "source": [
        "# 어텐션 디코더 모델 학습을 위한 함수\n",
        "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0\n",
        "    plot_loss_total = 0\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate) # 인코더와 디코더에 SGD 옵티마이저 적용\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    training_pairs = [tensorsFromPair(input_lang, output_lang, random.choice(pairs))\n",
        "                      for i in range(n_iters)]\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0] # 입출력 쌍에서 입력을 input_tensor로 사용\n",
        "        target_tensor = training_pair[1] # 입출력 쌍에서 출력을 target_tensor로 사용\n",
        "        loss = Model(model, input_tensor, target_tensor, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if iter % 5000 == 0: # 모델을 75000번 훈련을 진행하여 5000번째마다 오차를 출력\n",
        "            print_loss_avg = print_loss_total / 5000\n",
        "            print_loss_total = 0\n",
        "            print('%d,  %.4f' % (iter, print_loss_avg))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6df879d6",
      "metadata": {
        "id": "6df879d6"
      },
      "outputs": [],
      "source": [
        "# 어텐션 디코더 모델 훈련\n",
        "import time\n",
        "\n",
        "embed_size = 256\n",
        "hidden_size = 512\n",
        "num_layers = 1\n",
        "input_size = input_lang.n_words\n",
        "output_size = output_lang.n_words\n",
        "\n",
        "encoder1 = Encoder(input_size, hidden_size, embed_size, num_layers)\n",
        "attn_decoder1 = AttnDecoderRNN(hidden_size, output_size, dropout_p=0.1).to(device)\n",
        "\n",
        "print(encoder1)\n",
        "print(attn_decoder1)\n",
        "\n",
        "attn_model = trainIters(encoder1, attn_decoder1, 75000, print_every=5000, plot_every=100, learning_rate=0.01)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "U0PDIGJlKroW",
      "metadata": {
        "id": "U0PDIGJlKroW"
      },
      "source": [
        "## 10.2.2 버트(BERT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GalDYeCfJLJF",
      "metadata": {
        "id": "GalDYeCfJLJF"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DIsA0wggJZHI",
      "metadata": {
        "id": "DIsA0wggJZHI"
      },
      "outputs": [],
      "source": [
        "!pip install pytorch-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20b91e5b",
      "metadata": {
        "id": "20b91e5b"
      },
      "outputs": [],
      "source": [
        "# 라이브러리 호출\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "from pytorch_transformers import BertTokenizer, BertForSequenceClassification # 버트 사용을 위한 라이브러리\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix # 모델 평가를 위해 사용\n",
        "import seaborn as sns\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fa39b29",
      "metadata": {
        "id": "0fa39b29"
      },
      "outputs": [],
      "source": [
        "# 데이터셋 불러오기\n",
        "train_df = pd.read_csv('../chap10/data/training.txt', sep='\\t')\n",
        "valid_df = pd.read_csv('../chap10/data/validing.txt', sep='\\t')\n",
        "test_df = pd.read_csv('../chap10/data/testing.txt', sep='\\t')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e620c35f",
      "metadata": {
        "id": "e620c35f"
      },
      "outputs": [],
      "source": [
        "# 불러온 데이터셋 중 일부만 사용\n",
        "train_df = train_df.sample(frac=0.1, random_state=500)\n",
        "valid_df = valid_df.sample(frac=0.1, random_state=500)\n",
        "test_df = test_df.sample(frac=0.1, random_state=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8506022e",
      "metadata": {
        "id": "8506022e"
      },
      "outputs": [],
      "source": [
        "# 데이터셋 생성\n",
        "class Datasets(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.df = df\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.df.iloc[idx, 1]\n",
        "        label = self.df.iloc[idx, 2]\n",
        "        return text, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97277c2b",
      "metadata": {
        "id": "97277c2b"
      },
      "outputs": [],
      "source": [
        "# 데이터셋의 데이터를 데이터로더로 전달\n",
        "train_dataset = Datasets(train_df)\n",
        "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=0)\n",
        "\n",
        "valid_dataset = Datasets(valid_df)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=2, shuffle=True, num_workers=0)\n",
        "\n",
        "test_dataset = Datasets(test_df)\n",
        "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=True, num_workers=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "106307e9",
      "metadata": {
        "id": "106307e9"
      },
      "outputs": [],
      "source": [
        "# 버트 토크나이저 내려받기\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c0f5fef",
      "metadata": {
        "id": "7c0f5fef"
      },
      "outputs": [],
      "source": [
        "# 최적화 모델 저장\n",
        "def save_checkpoint(save_path, model, valid_loss): # 모델 평가를 위해 훈련 과정을 저장\n",
        "    if save_path == None:\n",
        "        return\n",
        "    state_dict = {'model_state_dict': model.state_dict(),\n",
        "                  'valid_loss': valid_loss}\n",
        "\n",
        "    torch.save(state_dict, save_path)\n",
        "    print(f'Model saved to ==> {save_path}')\n",
        "\n",
        "def load_checkpoint(load_path, model): # save_checkpoint 함수에서 저장된 모델을 가져옴\n",
        "    if load_path==None:\n",
        "        return\n",
        "    state_dict = torch.load(load_path, map_location=device)\n",
        "    print(f'Model loaded from <== {load_path}')\n",
        "\n",
        "    model.load_state_dict(state_dict['model_state_dict'])\n",
        "    return state_dict['valid_loss']\n",
        "\n",
        "def save_metrics(save_path, train_loss_list, valid_loss_list, global_steps_list): # 훈련, 검증에 대한 오차와 에포크를 저장\n",
        "    if save_path == None:\n",
        "        return\n",
        "    state_dict = {'train_loss_list': train_loss_list,\n",
        "                  'valid_loss_list': valid_loss_list,\n",
        "                  'global_steps_list': global_steps_list}\n",
        "    torch.save(state_dict, save_path)\n",
        "    print(f'Model saved to ==> {save_path}')\n",
        "\n",
        "def load_metrics(load_path): # save_metrics에 저장해 둔 정보를 불러옴\n",
        "    if load_path==None:\n",
        "        return\n",
        "    state_dict = torch.load(load_path, map_location=device)\n",
        "    print(f'Model loaded from <== {load_path}')\n",
        "    return state_dict['train_loss_list'], state_dict['valid_loss_list'], state_dict['global_steps_list']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0871e661",
      "metadata": {
        "id": "0871e661"
      },
      "outputs": [],
      "source": [
        "# 모델 훈련 함수 정의\n",
        "def train(model,\n",
        "          optimizer,\n",
        "          criterion = nn.BCELoss(), # 영화 리뷰는 좋고 나쁨만 있으므로 BinaryCrossEntropy를 사용\n",
        "          num_epochs = 5,\n",
        "          eval_every = len(train_loader) // 2,\n",
        "          best_valid_loss = float(\"Inf\")):\n",
        "\n",
        "    total_correct = 0.0\n",
        "    total_len = 0.0\n",
        "    running_loss = 0.0\n",
        "    valid_running_loss = 0.0\n",
        "    global_step = 0\n",
        "    train_loss_list = []\n",
        "    valid_loss_list = []\n",
        "    global_steps_list = []\n",
        "\n",
        "    model.train() # 모델 훈련\n",
        "    for epoch in range(num_epochs):\n",
        "        for text, label in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            encoded_list = [tokenizer.encode(t, add_special_tokens=True) for t in text]\n",
        "            padded_list =  [e + [0] * (512-len(e)) for e in encoded_list] # 인코딩 결과에 제로패딩 적용\n",
        "\n",
        "            sample = torch.tensor(padded_list)\n",
        "            sample, label = sample.to(device), label.to(device)\n",
        "            labels = torch.tensor(label)\n",
        "            outputs = model(sample, labels=labels)\n",
        "            loss, logits = outputs\n",
        "\n",
        "            pred = torch.argmax(F.softmax(logits), dim=1)\n",
        "            correct = pred.eq(labels)\n",
        "            total_correct += correct.sum().item()\n",
        "            total_len += len(labels)\n",
        "            running_loss += loss.item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            global_step += 1\n",
        "\n",
        "            if global_step % eval_every == 0: # 모델 평가\n",
        "                model.eval()\n",
        "                with torch.no_grad():\n",
        "                    for text, label in valid_loader:\n",
        "                        encoded_list = [tokenizer.encode(t, add_special_tokens=True) for t in text]\n",
        "                        padded_list =  [e + [0] * (512-len(e)) for e in encoded_list]\n",
        "                        sample = torch.tensor(padded_list)\n",
        "                        sample, label = sample.to(device), label.to(device)\n",
        "                        labels = torch.tensor(label)\n",
        "                        outputs = model(sample, labels=labels)\n",
        "                        loss, logits = outputs\n",
        "                        valid_running_loss += loss.item()\n",
        "\n",
        "                average_train_loss = running_loss / eval_every\n",
        "                average_valid_loss = valid_running_loss / len(valid_loader)\n",
        "                train_loss_list.append(average_train_loss)\n",
        "                valid_loss_list.append(average_valid_loss)\n",
        "                global_steps_list.append(global_step)\n",
        "\n",
        "                running_loss = 0.0\n",
        "                valid_running_loss = 0.0\n",
        "                model.train()\n",
        "\n",
        "                print('Epoch [{}/{}], Step [{}/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}'\n",
        "                      .format(epoch+1, num_epochs, global_step, num_epochs*len(train_loader),\n",
        "                              average_train_loss, average_valid_loss))\n",
        "\n",
        "                if best_valid_loss > average_valid_loss:\n",
        "                    best_valid_loss = average_valid_loss\n",
        "                    save_checkpoint('../chap10/data/model.pt', model, best_valid_loss) # 오차가 작아지면 모델 저장\n",
        "                    save_metrics('../chap10/data/metrics.pt', train_loss_list, valid_loss_list, global_steps_list) # 평가에 사용된 훈련 오차, 검증 오차, 에폭 저장\n",
        "\n",
        "    save_metrics('../chap10/data/metrics.pt', train_loss_list, valid_loss_list, global_steps_list) # 최종으로 사용된 훈련 오차, 검증 오차, 에폭 저장\n",
        "    print('훈련 종료!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f69dac7",
      "metadata": {
        "id": "7f69dac7"
      },
      "outputs": [],
      "source": [
        "# 모델의 파라미터(옵티마이저) 미세 조정 및 모델 훈련\n",
        "optimizer = optim.Adam(model.parameters(), lr=2e-5) # 아담과 적절한 학습률로 버트 모델을 미세 조정\n",
        "train(model=model, optimizer=optimizer) # 모델 학습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2abbb6cb",
      "metadata": {
        "id": "2abbb6cb"
      },
      "outputs": [],
      "source": [
        "# 오차 정보를 그래프로 확인\n",
        "train_loss_list, valid_loss_list, global_steps_list = load_metrics('../chap10/data/metrics.pt') # 최종으로 저장된 모델을 불러옴\n",
        "plt.plot(global_steps_list, train_loss_list, label='Train') # 훈련 데이터셋에 대한 오차\n",
        "plt.plot(global_steps_list, valid_loss_list, label='Valid') # 검증 데이터셋에 대한 오차\n",
        "plt.xlabel('Global Steps')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cb68692",
      "metadata": {
        "id": "0cb68692"
      },
      "outputs": [],
      "source": [
        "# 모델 평가 함수 정의\n",
        "def evaluate(model, test_loader):\n",
        "    y_pred = []\n",
        "    y_true = []\n",
        "\n",
        "    model.eval() # 테스트 데이터셋으로 모델 평가\n",
        "    with torch.no_grad():\n",
        "        for text, label in test_loader:\n",
        "            encoded_list = [tokenizer.encode(t, add_special_tokens=True) for t in text]\n",
        "            padded_list =  [e + [0] * (512-len(e)) for e in encoded_list]\n",
        "\n",
        "            sample = torch.tensor(padded_list)\n",
        "            sample, label = sample.to(device), label.to(device)\n",
        "            labels = torch.tensor(label)\n",
        "            output = model(sample, labels=labels)\n",
        "\n",
        "            _, output = output\n",
        "            y_pred.extend(torch.argmax(output, 1).tolist())\n",
        "            y_true.extend(labels.tolist())\n",
        "\n",
        "    print('Classification 결과:')\n",
        "    print(classification_report(y_true, y_pred, labels=[1,0], digits=4))\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=[1,0])\n",
        "    ax= plt.subplot()\n",
        "    sns.heatmap(cm, annot=True, ax = ax, cmap='Blues', fmt=\"d\")\n",
        "\n",
        "    ax.set_title('Confusion Matrix')\n",
        "    ax.set_xlabel('Predicted Labels')\n",
        "    ax.set_ylabel('True Labels')\n",
        "    ax.xaxis.set_ticklabels(['0', '1'])\n",
        "    ax.yaxis.set_ticklabels(['0', '1'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97dfe435",
      "metadata": {
        "id": "97dfe435"
      },
      "outputs": [],
      "source": [
        "# 모델 평가\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "best_model = model.to(device)\n",
        "load_checkpoint('../chap10/data/model.pt', best_model)\n",
        "evaluate(best_model, test_loader)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "Yy5LFujavCKn"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
