{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **4장 딥러닝 시작**"
      ],
      "metadata": {
        "id": "6levWAQMBw83"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 딥러닝 구조"
      ],
      "metadata": {
        "id": "AzUMh4nuSOgY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 활성화 함수\n",
        "- 렐루 함수와 소프트맥스 함수를 파이토치에서 구현하는 코드"
      ],
      "metadata": {
        "id": "rWaDXkTBSVmd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "8opNTcGzTR-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, n_feature, n_hidden, n_output):\n",
        "        super(Net, self).__init__()\n",
        "        self.hidden = torch.nn.Linear(n_feature, n_hidden)  # 은닉층\n",
        "        self.relu = torch.nn.ReLU(inplace=True)\n",
        "        self.out = torch.nn.Linear(n_hidden, n_output) # 출력층\n",
        "        self.softmax = torch.nn.Softmax(dim=n_output)\n",
        "    def forward(self, x):\n",
        "        x = self.hidden(x)\n",
        "        x = self.relu(x)  # 은닉층을 위한 렐루 활성화 함수\n",
        "        x = self.out(x)\n",
        "        x = self.softmax(x)  # 출력층을 위한 소프트맥스 활성화 함수\n",
        "        return x"
      ],
      "metadata": {
        "id": "KWAiHNkOSKjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 손실 함수"
      ],
      "metadata": {
        "id": "0Z3AAD-IkCO3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `평균 제곱 오차` 파이토치에서 사용하기\n",
        "\n",
        "\n",
        "```\n",
        "import torch\n",
        "\n",
        "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
        "y_pred = model(x)\n",
        "loss = loss_fn(y_pred, y)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "SRP06L-6kGJS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `크로스 엔트로피 오차` 파이토치에서 사용하기\n",
        "\n",
        "\n",
        "```\n",
        "loss = nn.CrossEntropy()\n",
        "input = torch.randn(5, 6, requires_grad=True)\n",
        "# torch.randn: 평균이 0, 표준편차 1인 가우시안 정규분포 이용하여 숫자 생성\n",
        "target = torch.empty(3, dtype=torch.long).random_(5)\n",
        "# torch.empty: dtype torch.float32의 랜덤한 값으로 채워진 텐서 반환\n",
        "output = loss(input, target)\n",
        "output.backward()\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "y0gqe5jokc_K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 딥러닝의 문제점과 해결방안\n",
        "- 과적합 문제 발생\n",
        "- 해결법 : 드롭아웃\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "# 드롭아웃을 구현하는 예시코드\n",
        "class DropoutModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DropoutModel, self).__init__()\n",
        "        self.layer1 = torch.nn.Linear(784, 1200)\n",
        "        self.dropout1 = torch.nn.Dropout(0.5) # 50%의 노드를 무작위로 선택하여 사용하지 않겠다는 의미\n",
        "        self.layer2 = torch.nn.Linear(1200, 1200)\n",
        "        self.dropout2 = torch.nn.Dropout(0.5)\n",
        "        self.layer3 = torch.nn.Linear(1200, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.layer1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.layer2(x))\n",
        "        x = self.dropout2(x)\n",
        "        return self.layer3(x)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "tZUeYrGrv3Gl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 성능 나빠지는 문제 발생\n",
        "- 해결법 : 미니 배치 경사 하강법 & 확률적 경사 하강법\n",
        "\n",
        "\n",
        "```\n",
        "# 미니 배치 경사 하강법 구현하는 코드\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self):\n",
        "        self.x_data = [[1,2,3],[4,5,6],[7,8,9]]\n",
        "        self.y_data = [[12],[18],[11]]\n",
        "        def __len__(self):\n",
        "            return len(self.x_data)\n",
        "        def __getitem__(self, idx):\n",
        "            x = torch.FloatTensor(self.x_data[idx])\n",
        "            y = torch.FloatTensor(self.y_data[idx])\n",
        "            return x, y\n",
        "dataset = CustomDataset()\n",
        "dataloader = DataLoader(\n",
        "    dataset,  # 데이터셋\n",
        "    batch_size = 2,  # 미니 배치 크기로 2의 제곱수 사용\n",
        "    shuffle = True, # 데이터를 불러올 때마다 랜덤으로 섞어서 가져오기\n",
        ")\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "BN8_Gu77-RZT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 옵티마이저\n",
        "- 확률적 경사 하강법의 변경 폭 불안정 문제 해결을 위해 옵티마이저 적용하기"
      ],
      "metadata": {
        "id": "vey9o8VjANGj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 속도를 조정하는 방법\n",
        "    - 아다그라드(Adagrad)\n",
        "```\n",
        "    optmizer = torch.optim.Adagrad(model.parameters(), lr=0.01) # 학습률 기본값 1e-2\n",
        "```\n",
        "\n",
        "    - 아다델타(Adadelta)\n",
        "```\n",
        "    optimizer = torch.optim.Adadelta(model.parameters(), lr=1.0) # 학습률 기본값 1.0\n",
        "```\n",
        "    - 암엠에스프롭(RMSProp)\n",
        "```\n",
        "    optimizer = torch.optim.RMSProp(model.parameters(), lr=0.01) # 학습률 기본값 1e-2\n",
        "```"
      ],
      "metadata": {
        "id": "kOEpNJNDAbSE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 운동량을 조정하는 방법\n",
        "    - 모멘텀(Momentum)\n",
        "```\n",
        "    optmizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "    # momentum 값은 0.9에서 시작하여 0.95, 0.99 조금씩 증가시키며 사용.\n",
        "```\n",
        "\n",
        "    - 네스테로프 모멘텀(Nesterov Accelerated Gradient, NAG)\n",
        "```\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, nesterov=True)\n",
        "    # nesterov 기본값은 Flase\n",
        "```"
      ],
      "metadata": {
        "id": "D918_hcADA7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 속도와 운동량에 대한 혼용 방법\n",
        "    - 아담(Adam)\n",
        "```\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01) # 학습률 기본값 1e-3\n",
        "```"
      ],
      "metadata": {
        "id": "86AsowxtGDU4"
      }
    }
  ]
}